{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### CIFAR10 Dataset 및 DataLoader 생성. Simple CNN 모델 생성","metadata":{"id":"DDhngRfatnnK"}},{"cell_type":"code","source":"from torchvision.datasets import CIFAR10\nfrom torchvision.transforms import ToTensor\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data import random_split\n\n#전체 6만개 데이터 중, 5만개는 학습 데이터용. 이를 다시 학습과 검증용으로 split , 1만개는 테스트 데이터용\ntrain_dataset = CIFAR10(root='./data', train=True, download=True, transform=ToTensor())\ntest_dataset = CIFAR10(root='./data', train=False, download=True, transform=ToTensor())\n\ntr_size = int(0.85 * len(train_dataset))\nval_size = len(train_dataset) - tr_size\ntr_dataset, val_dataset = random_split(train_dataset, [tr_size, val_size])\nprint('tr:', len(tr_dataset), 'valid:', len(val_dataset))\n\ntr_loader = DataLoader(tr_dataset, batch_size=32, shuffle=True, num_workers=4)\nval_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TUwnVEEuD1J9","outputId":"758d90e4-a822-4bd0-c511-4f27bb06f6a1","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n# from torchinfo import summary\n\nNUM_INPUT_CHANNELS = 3\n\nclass SimpleCNNWithBN(nn.Module):\n    def __init__(self, num_classes):\n        super().__init__()\n\n        #padding 1로 conv 적용 후 출력 면적 사이즈를 입력 면적 사이즈와 동일하게 유지.\n        self.conv_block_1 = self.create_convbn_block(first_channels=3, middle_channels=32, last_channels=32)\n\n        #out_channels이 64인 2개의 Conv2d. stride=1이 기본값, padding='same'은 version 1.8에서 소개됨.\n        self.conv_block_2 = self.create_convbn_block(first_channels=32, middle_channels=64, last_channels=64)\n\n        # filter갯수 128개인 Conv Layer 2개 적용 후 Max Pooling 적용.\n        self.conv_block_3 = self.create_convbn_block(first_channels=64, middle_channels=128, last_channels=128)\n\n        # GAP 및 최종 Classifier Layer\n        self.classifier_block = nn.Sequential(\n            nn.AdaptiveAvgPool2d(output_size=(1, 1)),\n            nn.Flatten(),\n            nn.Linear(in_features=128, out_features=num_classes)\n        )\n\n    def create_convbn_block(self, first_channels, middle_channels, last_channels):\n        conv_bn_block = nn.Sequential(\n            nn.Conv2d(in_channels=first_channels, out_channels=middle_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(middle_channels),\n            nn.ReLU(),\n            nn.Conv2d(in_channels=middle_channels, out_channels=last_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(last_channels),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2)\n        )\n        return conv_bn_block\n\n    def forward(self, x):\n        x = self.conv_block_1(x)\n        x = self.conv_block_2(x)\n        x = self.conv_block_3(x)\n        x = self.classifier_block(x)\n\n        return x\n\ndef create_do_classifier_block(first_features, second_features, first_dos, second_dos, num_classes=10):\n    return nn.Sequential(\n            nn.Flatten(),\n            nn.Dropout(p=first_dos),\n            nn.Linear(in_features=first_features, out_features=second_features),\n            nn.ReLU(),\n            nn.Dropout(p=second_dos),\n            nn.Linear(in_features=second_features, out_features=num_classes),\n        )\n","metadata":{"id":"iaHYYU-vJAQw","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 다양한 Learning Rate Scheduler\n* pytorch는 nn.optim.lr_scheduler에서 다양한 유형의 Learning Rate Scheduler 클래스를 지원\n* 검증 지표를 입력 받고, 조건에 맞춰서 epochs시 마다 Learning Rate를 변경하는 Scheduler와 검증 지표를 입력 받지 않고 epochs 반복 시 특정 패턴으로 Learning Rate를 변경하는 Scheduler 가 있음.\n* ReduceLROnPlateur는 (주로) 검증 데이터를 기반의 평가 지표(loss등)를 사용하여 수행. 검증 평가 수치를 epoch시마다 모니터링 하면서 지속적으로 나빠지면 Learning rate를 training oop의 새로운 epoch시에 변경함.\n* StepLR, CycleLR, CosineAnnealingLR 등은 설정된 패턴에 따라 Learning Rate를 주기적으로 변경","metadata":{}},{"cell_type":"markdown","source":"### Learning Rate Scheduler 생성 및 적용.\n* Learning Rate Scheduler는 초기 생성 시 Optimizer 객체를 요구하는 데, 이는 optimzer에 의해서 관리되는 learning 관련 파라미터를 바로 접근하기 위해서임. Learning rate 변경은 Optimizer를 통해서 이뤄져야 함.\n* LR Scheduler는 scheduler.step() 와 같이 step() 메소드를 호출하여 learning rate의 Scheduler를 변경.\n* scheduler.step()은 epoch 마다 호출이 되어야 하며(배치 단위 아님) optimizer.step()이후에 호출 \n* Learning rate정보는 optimizer에서 optimizer.param_groups[0]['lr'] 로도 볼수 있고, scheduler.get_last_lr()[0] 또는 scheduler.get_lr()[0] 로 볼 수 있음.\n* scheduler.get_last_lr()[0]는 scheduler.step()으로 적용된 learning rate를, scheduler.get_lr()[0]는 scheduler.step()를 호출 시 다음에 적용될 learning rate를 반환.  ","metadata":{"id":"IuntH3DmQlvN"}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.optim import SGD, Adam\nfrom torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau\n\nNUM_CLASSES = 10\n\nsample_model = SimpleCNNWithBN(num_classes=NUM_CLASSES)\noptimizer = Adam(sample_model.parameters(), lr=0.001)\nscheduler = StepLR(optimizer, step_size=1, gamma=0.5)\n\nprint('optimizer lr:', optimizer.param_groups[0]['lr'])#optimizer.param_groups[0].keys()\nprint('get_last_lr():', scheduler.get_last_lr()[0])\nprint('get_lr():', scheduler.get_lr()[0])\n\nscheduler.step()\n\nprint('optimizer lr:', optimizer.param_groups[0]['lr'])#optimizer.param_groups[0].keys()\nprint('get_last_lr():', scheduler.get_last_lr()[0])\nprint('get_lr():', scheduler.get_lr()[0])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Learning Rate Scheduler를 적용하도록 Trainer 클래스를 변경\n* scheduler.step()은 epoch 마다 호출이 되어야 하며(배치 단위 아님) optimizer.step()이후에 호출 \n* ReduceLROnPlateur 는 (주로) Validation epoch에 적용되며 scheduler.step(val_loss)와 같이 step()메소드의 인자로 평가 수치 정보를 입력해 줌.\n* 다른 LR Scheduler는 (주로) Train epoch에 적용되며, scheduler.step()을 호출하여 수행됨.","metadata":{}},{"cell_type":"code","source":"from tqdm import tqdm\nimport torch.nn.functional as F\n\nclass Trainer:\n    def __init__(self, model, loss_fn, optimizer, train_loader, val_loader, scheduler=None, device=None):\n        self.model = model.to(device)\n        self.loss_fn = loss_fn\n        self.optimizer = optimizer\n        self.train_loader = train_loader\n        self.val_loader = val_loader\n        # scheduler 추가\n        self.scheduler = scheduler\n        self.device = device\n        # 현재 learning rate 변수 추가\n        self.current_lr = self.optimizer.param_groups[0]['lr']\n\n    def train_epoch(self, epoch):\n        self.model.train()\n\n        # running 평균 loss 계산.\n        accu_loss = 0.0\n        running_avg_loss = 0.0\n        # 정확도, 정확도 계산을 위한 전체 건수 및 누적 정확건수\n        num_total = 0.0\n        accu_num_correct = 0.0\n        accuracy = 0.0\n        # tqdm으로 실시간 training loop 진행 상황 시각화\n        with tqdm(total=len(self.train_loader), desc=f\"Epoch {epoch+1} [Training..]\", leave=True) as progress_bar:\n            for batch_idx, (inputs, targets) in enumerate(self.train_loader):\n                # 반드시 to(self.device). to(device) 아님.\n                inputs = inputs.to(self.device)\n                targets = targets.to(self.device)\n\n                # Forward pass\n                outputs = self.model(inputs)\n                loss = self.loss_fn(outputs, targets)\n\n                # Backward pass\n                self.optimizer.zero_grad()\n                loss.backward()\n                self.optimizer.step()\n\n                # batch 반복 시 마다 누적  loss를 구하고 이를 batch 횟수로 나눠서 running 평균 loss 구함.\n                accu_loss += loss.item()\n                running_avg_loss = accu_loss /(batch_idx + 1)\n\n                # accuracy metric 계산\n                # outputs 출력 예측 class값과 targets값 일치 건수 구하고\n                num_correct = (outputs.argmax(-1) == targets).sum().item()\n                # 배치별 누적 전체 건수와 누적 전체 num_correct 건수로 accuracy 계산  \n                num_total += inputs.shape[0]\n                accu_num_correct += num_correct\n                accuracy = accu_num_correct / num_total\n\n                #tqdm progress_bar에 진행 상황 및 running 평균 loss와 정확도 표시\n                progress_bar.update(1)\n                if batch_idx % 20 == 0 or (batch_idx + 1) == progress_bar.total:  # 20 batch 횟수마다 또는 맨 마지막 batch에서 update\n                    progress_bar.set_postfix({\"Loss\": running_avg_loss,\n                                              \"Accuracy\": accuracy})\n\n        if (self.scheduler is not None) and (not isinstance(self.scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau)):\n            self.scheduler.step()\n            self.current_lr = self.scheduler.get_last_lr()[0]\n            print(\"scheduler step() call\")\n\n        return running_avg_loss, accuracy\n\n    def validate_epoch(self, epoch):\n        if not self.val_loader:\n            return None\n\n        self.model.eval()\n\n        # running 평균 loss 계산.\n        accu_loss = 0.0\n        running_avg_loss = 0.0\n        # 정확도, 정확도 계산을 위한 전체 건수 및 누적 정확건수\n        num_total = 0.0\n        accu_num_correct = 0.0\n        accuracy = 0.0\n        \n        with tqdm(total=len(self.val_loader), desc=f\"Epoch {epoch+1} [Validating]\", leave=True) as progress_bar:\n            with torch.no_grad():\n                for batch_idx, (inputs, targets) in enumerate(self.val_loader):\n                    inputs = inputs.to(self.device)\n                    targets = targets.to(self.device)\n\n                    outputs = self.model(inputs)\n\n                    loss = self.loss_fn(outputs, targets)\n                    # batch 반복 시 마다 누적  loss를 구하고 이를 batch 횟수로 나눠서 running 평균 loss 구함.\n                    accu_loss += loss.item()\n                    running_avg_loss = accu_loss /(batch_idx + 1)\n\n                    # accuracy metric 계산\n                    # outputs 출력 예측 class값과 targets값 일치 건수 구하고\n                    num_correct = (outputs.argmax(-1) == targets).sum().item()\n                    # 배치별 누적 전체 건수와 누적 전체 num_correct 건수로 accuracy 계산  \n                    num_total += inputs.shape[0]\n                    accu_num_correct += num_correct\n                    accuracy = accu_num_correct / num_total\n\n                    #tqdm progress_bar에 진행 상황 및 running 평균 loss와 정확도 표시\n                    progress_bar.update(1)\n                    if batch_idx % 20 == 0 or (batch_idx + 1) == progress_bar.total:  # 20 batch 횟수마다 또는 맨 마지막 batch에서 update\n                        progress_bar.set_postfix({\"Loss\": running_avg_loss,\n                                                  \"Accuracy\":accuracy})\n        # scheduler에 검증 데이터 기반에서 epoch레벨로 계산된 loss를 입력해줌.\n        if (self.scheduler is not None) and isinstance(self.scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n            self.scheduler.step(running_avg_loss)\n            self.current_lr = self.scheduler.get_last_lr()[0]\n            print(\"scheduler step(evaluation_value) call\")\n\n        return running_avg_loss, accuracy\n\n    def fit(self, epochs):\n        # epoch 시마다 학습/검증 결과를 기록하는 history dict 생성. learning rate 추가\n        history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': [], 'lr': []}\n        for epoch in range(epochs):\n            train_loss, train_acc = self.train_epoch(epoch)\n            val_loss, val_acc = self.validate_epoch(epoch)\n            print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f} Train Accuracy: {train_acc:.4f}\",\n                  f\", Val Loss: {val_loss:.4f} Val Accuracy: {val_acc:.4f}\" if val_loss is not None else \"\",\n                  f\", Current lr:{self.current_lr:.6f}\")\n            # epoch 시마다 학습/검증 결과를 기록. learning rate 추가\n            history['train_loss'].append(train_loss); history['train_acc'].append(train_acc)\n            history['val_loss'].append(val_loss); history['val_acc'].append(val_acc)\n            history['lr'].append(self.current_lr)\n\n        return history\n\n    # 학습이 완료된 모델을 return\n    def get_trained_model(self):\n        return self.model","metadata":{"id":"DssSb_bNJLUk","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Learning Rate Scheduler가 적용된 Trainer를 이용하여 모델 학습\n* ReduceLROnPlateur의 주요 생성 인자\n  * mode: 'min'또는 'max'이며 min은 값이 작아질 수록 개선 되는 지표(예: loss), max는 값이 커질 수록 개선되는 지표(예: 정확도)\n  * factor: learning rate 변경 적용 시 곱해지는 scale 값.  new_lr = lr * factor. 기본은 0.1\n  * patience: 학습률 감소 결정 전 지표가 더 이상 나아지지 않는 epoch 횟수. patience 수를 넘길 때 까지 지표 개선되지 않으면 감소\n  * threshold: patience에서 무시해도 될 정도의 작은 지표 변경값\n  * min_lr: 더 이상 감소하지 않을 최소 학습률\n  * cooldown: 한번 학습률 감소 후 다시 정상적으로 감소 판별 로직을 수행하기 전까지 기다리는 epoch 횟수","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.optim import SGD, Adam\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\n\nNUM_INPUT_CHANNELS = 3\nNUM_CLASSES = 10\n\nmodel = SimpleCNNWithBN(num_classes=NUM_CLASSES)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\noptimizer = Adam(model.parameters(), lr=0.001)\nloss_fn = nn.CrossEntropyLoss()\nscheduler = ReduceLROnPlateau(\n            optimizer=optimizer, mode='min', factor=0.5, patience=2, threshold=0.01, min_lr=0.00001)\n\ntrainer = Trainer(model=model, loss_fn=loss_fn, optimizer=optimizer,\n       train_loader=tr_loader, val_loader=val_loader, scheduler=scheduler, device=device)\n# 학습 및 평가\nhistory = trainer.fit(30)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":431},"id":"xWhl6g_0IkTZ","outputId":"561b0e6b-9a0e-4635-c7a4-4efd043dccb3","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class Predictor:\n    def __init__(self, model, device):\n        self.model = model.to(device)\n        self.device = device\n\n    def evaluate(self, loader):\n        self.model.eval()\n        eval_metric = 0.0\n        # 정확도 계산을 위한 전체 건수 및 누적 정확건수\n        num_total = 0.0\n        accu_num_correct = 0.0\n\n        with tqdm(total=len(loader), desc=f\"[Evaluating]\", leave=True) as progress_bar:\n            with torch.no_grad():\n                for batch_idx, (inputs, targets) in enumerate(loader):\n                    inputs = inputs.to(self.device)\n                    targets = targets.to(self.device)\n                    pred = self.model(inputs)\n\n                    # 정확도 계산을 위해 누적 전체 건수와 누적 전체 num_correct 건수 계산  \n                    num_correct = (pred.argmax(-1) == targets).sum().item()\n                    num_total += inputs.shape[0]\n                    accu_num_correct += num_correct\n                    eval_metric = accu_num_correct / num_total\n\n                    progress_bar.update(1)\n                    if batch_idx % 20 == 0 or (batch_idx + 1) == progress_bar.total:\n                        progress_bar.set_postfix({\"Accuracy\": eval_metric})\n        \n        return eval_metric\n\n    def predict_proba(self, inputs):\n        self.model.eval()\n        with torch.no_grad():\n            inputs = inputs.to(self.device)\n            outputs = self.model(inputs)\n            #예측값을 반환하므로 targets은 필요 없음.\n            #targets = targets.to(self.device)\n            pred_proba = F.softmax(outputs, dim=-1) #또는 dim=1\n\n        return pred_proba\n\n    def predict(self, inputs):\n        pred_proba = self.predict_proba(inputs)\n        pred_class = torch.argmax(pred_proba, dim=-1)\n\n        return pred_class","metadata":{"id":"CPvNs1u_IdXS","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trained_model = trainer.get_trained_model()\n\n# 학습데이터와 동일하게 정규화된 데이터를 입력해야 함.\n# test_dataset = CIFAR10(root='./data', train=False, download=True, transform=ToTensor())\n# test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)\n\npredictor = Predictor(model=trained_model, device=device)\neval_metric = predictor.evaluate(test_loader)\nprint(f'test dataset evaluation:{eval_metric:.4f}')","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QdRSo5S2Ep88","outputId":"7cd6fca3-f5a2-45f5-b158-da920fd34eec","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 다양한 Learning Rate Scheduler\n* StepLR, CycleLR, CosineAnnealingLR, CosineAnnealingWarmRestarts 등의 동작방식 이해 ","metadata":{"id":"8YtdBg2qZesC"}},{"cell_type":"markdown","source":"#### StepLR\n* 특정 Step시 마다 Learning Rate를 감소 시킴","metadata":{"id":"ff1eCoU7bH9x"}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\n\n# 모델 정의\nmodel = nn.Linear(10, 1)\noptimizer = optim.Adam(model.parameters(), lr=0.1)\n# 학습률을 epoch마다 저장.\nlr_history = []\n# StepLR 생성\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.5)\n\n# training loop 시뮬레이션\nprint(\"Epoch\\tLearning Rate\")\nfor epoch in range(20):\n    print(f\"{epoch+1}\\t{scheduler.get_last_lr()[0]:.6f}\")\n    lr_history.append(scheduler.get_last_lr()[0])\n    scheduler.step()\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dRY5syw7cmge","outputId":"d9dfae82-f4a1-43ef-b699-76ec1ff8f9c9","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nplt.figure(figsize=(4, 3)), \nx_label = [ index+1 for index in range(len(lr_history))]\nplt.plot(x_label, lr_history, marker='o')\nplt.xlabel('Epochs Number (Index starts from 1)')\nplt.ylabel('Learning Rate')\n\nplt.grid(True)\nplt.show()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":291},"id":"tO4od-xSdL-_","outputId":"17840a64-2bcd-4cc2-a312-4743be5841d1","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ndef get_lr_history(scheduler, epochs=30, verbose=False):\n  lr_history = []\n\n  # Simulate training loop\n  if verbose:\n    print(\"Epoch\\tLearning Rate\")\n  for epoch in range(epochs):\n    if verbose:\n      print(f\"{epoch+1}\\t{scheduler.get_last_lr()[0]:.6f}\")\n    lr_history.append(scheduler.get_last_lr()[0])\n    scheduler.step()\n  return lr_history\n\ndef draw_lr_history(lr_history, figsize=(6, 3)):\n    plt.figure(figsize=figsize)\n    x_label = [ index+1 for index in range(len(lr_history))]\n    plt.plot(x_label, lr_history,  marker='o')\n    \n    plt.xlabel('Epochs Number (Index starts from 1)')\n    plt.ylabel('Learning Rate')\n    \n    plt.grid(True)\n    plt.show()\n\nmodel = nn.Linear(10, 1)\noptimizer = optim.Adam(model.parameters(), lr=0.1)\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.5)\n\nlr_history = get_lr_history(scheduler=scheduler)\ndraw_lr_history(lr_history)\n","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":348},"id":"zBjGYRGbfi7p","outputId":"03546900-5a2e-4f22-d3e5-d5c4693f7fd5","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### MultiStepLR\n* milestones에 지정된 step별로 learning rate를 조정","metadata":{"id":"v1P5zC5ViDN3"}},{"cell_type":"code","source":"model = nn.Linear(10, 1)\noptimizer = optim.SGD(model.parameters(), lr=0.1)\nscheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[5, 8, 12], gamma=0.5)\n\nlr_history = get_lr_history(scheduler=scheduler) #verbose=True\ndraw_lr_history(lr_history)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":735},"id":"Hw_tW2v0hk7x","outputId":"ddbf2db4-c78f-4a15-d273-bbd1109ec02f","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### CycleLR\n* 학습률이 최소에서 최대로 반복적으로 변함. 학습률(eta)값이 증가/감소를 반복 수행.\n* base_lr: 최소 학습률\n* max_lr: 최대 학습률\n* step_size_up: 최대 학습률에 이르는 epoch 횟수\n* step_size_down: 최소 학습률에 이르는 epoch 횟수\n* mode: 학습률 cycle의 형태. triangular, triangular2, exp_range가 있음\n    * triangular: 삼각파형. 반복 시에도 최대/최소 학습률이 동일\n    * triangular2: 삼각파형인데, 반복 시마다 최대 학습률이 절반으로 감소함\n    * exp_range: 지수감소형이며, gamma 값 factor레벨로 epoch시마다 학습률이 감소\n    * gamma: exp_range에서 적용될 학습률 감소 factor값","metadata":{}},{"cell_type":"code","source":"model = nn.Linear(10, 1)\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n# base_lr은 optimizer의 lr가 동일하게 설정.\nscheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=0.001, max_lr=0.01,\n                                            step_size_up=5,  mode='exp_range', gamma=0.9)\n# scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=0.001, max_lr=0.01,\n#                                             step_size_up=10, mode=\"exp_range\", gamma=0.9)\nlr_history = get_lr_history(scheduler=scheduler, verbose=True)\ndraw_lr_history(lr_history)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":291},"id":"_l3pg4k4hkjh","outputId":"9343e416-5029-455d-8f5d-a308b68ee32f","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### CosineAnnealing\n* Cosine선분 형태로 학습률을 최대에서 최소로 점진적으로 감소하는데, 이 Cosine 선분을 주기적으로 반복함.\n    * 최대 학습률 -> 최소 학습률로 Cosine 선분 형태로 감소. 다시 최대 학습률-> 최소 학습률로 Cosine 반복\n* T_max: 최대->최소 학습률에 이르는 epochs 횟수.\n* eta_min: 최소 학습률","metadata":{}},{"cell_type":"code","source":"model = nn.Linear(10, 1)\noptimizer = optim.SGD(model.parameters(), lr=0.001)\n# 시작은 optimizer의 최초 learning rate, eta_min은 가장 최소 learning rate\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10, eta_min=0.0001)\nlr_history = get_lr_history(scheduler=scheduler, epochs=60)\ndraw_lr_history(lr_history, figsize=(12, 4))","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":291},"id":"k7_em6QMwixR","outputId":"9b46af75-1087-4756-a610-e7132cf8a1c7","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### CosineAnnealingWarmRestarts\n* CosineAnnealing과 유사하지만, cosine iteration 마다 주기적으로 epoch가 늘어나며 최소 eta에서 최대 eta로 학습률을 바로 높임\n* T_0: 첫번째 Cosine iteration 시 최소 학습률(eta)까지 떨어지는 epochs 횟수\n* T_mult: Cosine iteration을 반복 시 마다 늘어나는 epochs 횟수 factor.\n    * T_0가 10, T_mult가 2라면 첫번째 Cos iter'는 10회에서 최소 eta,  두번째 Cos iter'는 30회에서 최소 eta\n* eta_min: 최소 eta(학습률)\n","metadata":{}},{"cell_type":"code","source":"model = nn.Linear(10, 1)\noptimizer = optim.Adam(model.parameters(), lr=0.001)\nscheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2, eta_min=0.0001) #T_0를 20으로 변경\nlr_history = get_lr_history(scheduler=scheduler, epochs=150)\ndraw_lr_history(lr_history, figsize=(12, 4))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### CosineAnnealingWarmRestarts을 CNN 모델에 적용","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.optim import SGD, Adam\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\n\nNUM_INPUT_CHANNELS = 3\nNUM_CLASSES = 10\n\nmodel = SimpleCNNWithBN(num_classes=NUM_CLASSES)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\noptimizer = Adam(model.parameters(), lr=0.001)\nloss_fn = nn.CrossEntropyLoss()\n#scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=20, eta_min=0.0001) # T_max=10로 변경 후 테스트\nscheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2, eta_min=0.0001)\n# scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=0.001, max_lr=0.01, step_size_up=10, mode='exp_range', gamma=0.9)\ntrainer = Trainer(model=model, loss_fn=loss_fn, optimizer=optimizer,\n       train_loader=tr_loader, val_loader=val_loader, scheduler=scheduler, device=device)\n# 학습 및 평가\nhistory = trainer.fit(60) # fit을 30에서 60으로 변경. ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trained_model = trainer.get_trained_model()\n\n# 학습데이터와 동일하게 정규화된 데이터를 입력해야 함.\n# test_dataset = CIFAR10(root='./data', train=False, download=True, transform=ToTensor())\n# test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)\n\npredictor = Predictor(model=trained_model, device=device)\neval_metric = predictor.evaluate(test_loader)\nprint(f'test dataset evaluation:{eval_metric:.4f}')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Weight Decay\n* 가중치 규제(Weight Regularization)를 적용. 값을 키우면 오버피팅을 개선할 수 있지만, 언더 피팅을 할 수 있음. 값이 너무 작으면 오버피팅 개선 효과가 작아짐.\n* Optimizer의 생성 인자에 weight_decay를 설정해줌. 보통 1e-2 ~ 1e-3 사이 값을 적용\n* weight decay를 적용 시 Adam 보다는 AdamW에 적용하는 것이 효과적. ","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.optim import SGD, Adam, AdamW\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\n\nNUM_INPUT_CHANNELS = 3\nNUM_CLASSES = 10\n\nmodel = SimpleCNNWithBN(num_classes=NUM_CLASSES)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\noptimizer = AdamW(model.parameters(), lr=0.001, weight_decay=1e-2) #weight_decay=1e-3 \nloss_fn = nn.CrossEntropyLoss()\n\nscheduler = ReduceLROnPlateau(\n            optimizer=optimizer, mode='min', factor=0.5, patience=2, threshold=0.01, min_lr=0.00001)\n\ntrainer = Trainer(model=model, loss_fn=loss_fn, optimizer=optimizer,\n       train_loader=tr_loader, val_loader=val_loader, scheduler=scheduler, device=device)\n\nhistory = trainer.fit(30)","metadata":{"id":"aHoKILEBlJGO","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trained_model = trainer.get_trained_model()\n\n# 학습데이터와 동일하게 정규화된 데이터를 입력해야 함.\n# test_dataset = CIFAR10(root='./data', train=False, download=True, transform=ToTensor())\n# test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)\n\npredictor = Predictor(model=trained_model, device=device)\neval_metric = predictor.evaluate(test_loader)\nprint(f'test dataset evaluation:{eval_metric:.4f}')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}