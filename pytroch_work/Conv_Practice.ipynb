{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Conv2d 적용하기\n* Conv2d Layer의 주요 생성 인자\n    * in_channels: 입력 tensor의 channel 수(차원)\n    * out_channels: conv 연산 적용 후 생성되는 출력 tensor(output feature map)의 차원 수\n    * kernel_size: conv kernel size. (5, 5)와 같은 튜플 형태(이 경우 5x5 kernel size) 또는 5와 같이 정수값\n    * stride: conv연산 stride\n    * padding: conv 연산 전 입력 데이터의 상하좌우로 채우는 빈 값 size(output feature map의 사이즈 크기 조정을 위해 사용)","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\ninput_tensor = torch.randn(3, 28, 28)\nprint('input tensor shape:', input_tensor.shape)\n\nconv_layer_01 = nn.Conv2d(in_channels=3, out_channels=8, kernel_size=5, stride=1)\noutput_tensor = conv_layer_01(input_tensor)\nprint('output tensor shape:', output_tensor.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"conv_layer_01 = nn.Conv2d(in_channels=3, out_channels=8, kernel_size=5, stride=1, padding=0)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#conv2d layer weight shape\nconv_layer_01.weight.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"input_tensor = torch.randn(3, 28, 28)\n\n# 2개의 convolution을 적용하여 최종 output의 shape가 (16, 22, 22)가 나옴. \nconv_layer_01 = nn.Conv2d(in_channels=3, out_channels=8, kernel_size=5, stride=1) #padding='same'\nconv_layer_02 = nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3, stride=1) #padding='same'\noutput_01 = conv_layer_01(input_tensor)\noutput_02 = conv_layer_02(output_01)\n\nprint('output_01 shape:', output_01.shape, 'output_02 shape:', output_02.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"conv_layer_01.weight.shape, conv_layer_02.weight.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### MaxPool2d(AvgPool2d) 적용\n* MaxPool2d은 kernel_size 만큼 window 이동으로 가장 큰 값을 추출해 가면 output을 생성.\n* Pooling은 학습 파라미터를 가지고 있지 않은 Layer\n* 주요 기능\n  * 입력 feature map의 사이즈를 줄여서 computational cost 감소\n  * 차원 축소의 역할로서 특정 영역별로(kernel_size) 입력 feature map을 주요한 feature값으로 요약\n  * 입력값이 작은 변화에 너무 민감하게 반응하지 않도록 변동성 감소(overfitting 감소)\n* kernel_size, stride, padding을 생성 파라미터로 가짐. stride의 default값이 kernel_size 값임에 유의","metadata":{}},{"cell_type":"code","source":"input_tensor = torch.randn(3, 28, 28)\n\nconv_layer_01 = nn.Conv2d(in_channels=3, out_channels=12, kernel_size=3, stride=1, padding=1)\npool_layer_01 = nn.MaxPool2d(kernel_size=2)# stride는 기재하지 않으면 kernel_size와 동일. \noutput_01 = conv_layer_01(input_tensor)\noutput_02 = pool_layer_01(output_01)\n\nprint('output_01 shape:', output_01.shape, 'output_02 shape:', output_02.shape)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### CNN 기반 모델 생성 - 01\n* Conv2d 기반으로 모델 생성. Conv2d -> ReLU -> Conv2d -> ReLU -> MaxPool2d 로 CNN 모델 생성\n* 마지막 classification layer는 Linear Layer가 되어야 하므로 Conv2d 수행 결과인 3차원 Feature Map을 flatten하여 Linear Layer 연결 필요. 이를 위해 Flatten을 Feature map에 적용한 뒤 Linear Layer로 연결\n* 마지막 Feature Map -> Flatten -> Linear 적용을 하게 되면 Linear에 매우 많은 Learnable Parameter 를 가지게 됨(Overfitting의 이슈 발생하기 쉬움. Drop out등의 설정 필요 할 수 있음)\n* Linear Layer의 입력 in_features는 Flatten의 결과로 만들어진 입력의 갯수를 설정해줘야 하지만, 이미지 크기나 Conv 설정에 따라 변하게 됨.이를 위해 최종 feature map의 크기를 식으로 계산하거나, summary등을 통해 미리 파악 후 적용 필요. 하지만 이미지 크기나 Conv 설정이 변경되면 다시 계산해야 하는 불편함이 있음.\n  ","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchinfo import summary\n\nNUM_INPUT_CHANNELS = 3\n\n# 3x3 kernel, 32개의 filter들을 가지는 Conv Layer, 3x3 kernel, 64개의 filter들을 가지는 Conv Layer, 이후 MaxPooling \nclass SimpleCNN_01(nn.Module):\n    def __init__(self, num_classes=10):\n        super().__init__()\n        self.conv_1 = nn.Conv2d(in_channels=NUM_INPUT_CHANNELS, out_channels=32, kernel_size=3, stride=1)\n        self.conv_2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1)\n        self.pool = nn.MaxPool2d(kernel_size=2)\n        self.flatten = nn.Flatten()\n        self.classifier = nn.Linear(in_features=12544, out_features=num_classes)\n\n    def forward(self, x):\n        x = F.relu(self.conv_1(x))\n        x = F.relu(self.conv_2(x))\n        x = self.pool(x)\n        x = self.flatten(x)\n        x = self.classifier(x)\n\n        return x\n\ninput = torch.randn(1, 3, 32, 32) # 이미지 사이즈를 64, 64로 변경하면 classfication layer 오류 발생.\nsimple_cnn_01 = SimpleCNN_01(num_classes=10)\noutput = simple_cnn_01(input)\nprint(output.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torchinfo import summary\n\nsummary(model=simple_cnn_01, input_size=(1, 3, 32, 32),\n        col_names=['input_size', 'output_size', 'num_params'],\n        row_settings=['var_names'])\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### AdaptiveAvgPool2d를 이용한 Global Average Pooling\n* Pytorch에서는 GAP를 위해서 AdaptiveAvgPool2d()를 적용\n* AdaptiveAvgPool2d는 인자로 output_size를 받으며, channel별로 자동으로 지정된 output_size가 되도록 subsampling 수행. ","metadata":{}},{"cell_type":"code","source":"input = torch.randn(1, 64, 8, 9)\n\n# 만들어지는 (채널별)출력 size는 5x5\nm = nn.AdaptiveAvgPool2d(output_size=(5, 5))\noutput = m(input)\nprint(output.shape)\n\nm = nn.AdaptiveAvgPool2d(output_size=(1, 1))\noutput = m(input)\nprint(output.shape)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### CNN 기반 모델 생성 - 02\n* Feature map을 바로 Flatten하지 않고 Adaptive Global Pooling을 적용한 뒤 Flatten 적용\n* Global Pooling은 MaxPool2d와 다르게 채널별로 하나의 값으로 Pooling을 적용할 수 있음. 보통은 AdaptiveAvgPool2d가 많이 활용됨.\n* 마지막 Feature Map에 AdaptiveAvgPool2d(output_size=(1, 1))을 적용하면 feature map의 채널수는 동일하지만 면적(가로와 세로)은 1인 feature map으로 Pooling됨. 때문에 마지막 Conv2d의 out_channels 수만 알면 Flatten을 생기는 차원을 알 수 있으며, 이를 Linear Layer의 in_features의 값으로 입력하면 됨.\n* Global Pooling은 Feature Map 압축 효과로 인하여 classification Layer의 파라미터를 크게 줄일 수 있음.\n* Global Pooling은 보통 CNN의 Layer가 어느정도 깊이가 있어야 성능 저하가 발생하지 않음. ","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchinfo import summary\n\nNUM_INPUT_CHANNELS = 3\n\nclass SimpleCNN_02(nn.Module):\n    def __init__(self, num_classes):\n        super().__init__()\n        self.conv_1 = nn.Conv2d(in_channels=NUM_INPUT_CHANNELS, out_channels=32, kernel_size=3, stride=1)\n        self.conv_2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1)\n        self.pool = nn.MaxPool2d(kernel_size=2)\n        self.adapt_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))\n                \n        # in_features는 마지막 Conv2d의 out_channels임.\n        self.classifier = nn.Linear(in_features=64, out_features=num_classes)\n\n    def forward(self, x):\n        x = F.relu(self.conv_1(x))\n        x = F.relu(self.conv_2(x))\n        x = self.pool(x)\n\n        # Global Pooling 적용. \n        x = self.adapt_pool(x)\n        x = x.view(x.size(0), -1) #x = torch.flatten(x, start_dim=1)\n        x = self.classifier(x)\n\n        return x\n        \ninput = torch.randn(1, 3, 64, 64) # 이미지 사이즈를 64, 64로 변경해도 classfication layer 오류 없음.\nsimple_cnn_02 = SimpleCNN_02(num_classes=10)\noutput = simple_cnn_02(input)\nprint(output.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"simple_cnn_02 = SimpleCNN_02(num_classes=10)\nsummary(model=simple_cnn_02, input_size=(1, 3, 32, 32), # 이미지 사이즈를 64, 64로 변경해도 오류 발생하지 않음.  \n        col_names=['input_size', 'output_size', 'num_params'], \n        row_settings=['var_names'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torchvision.models as models\n\n# torchvision의 pretrained 모델 구조에서 Global Average Pooling 적용 살펴 보기\nmodel = models.vgg19() # models.resnet50()\nprint(model)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### CIFAR10 Dataset 생성\n* torchvision.datasets의 CIFAR10으로 dataset 생성. transform=ToTensor() 수행 시 PIL image를 tensor로 변환하면서 0 ~ 1사이 값으로 Normalization 적용.\n* CIFAR10 Dataset의 data 속성은 numpy 형태로 이미지값을 가짐. targets 속성은 np.uint8 형태로 target값을 가짐. classes 속성은 개별 target에 매핑되는 class의 이름을 가짐.  \n* CIFAR10 Dataset 기반으로 DataLoader 생성.","metadata":{}},{"cell_type":"code","source":"from torchvision.datasets import CIFAR10\nfrom torchvision.transforms import ToTensor\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data import random_split\n\n#전체 6만개 데이터 중, 5만개는 학습 데이터용. 이를 다시 학습과 검증용으로 split , 1만개는 테스트 데이터용\ntrain_dataset = CIFAR10(root='./data', train=True, download=True, transform=ToTensor())\ntest_dataset = CIFAR10(root='./data', train=False, download=True, transform=ToTensor())\n\ntr_size = int(0.85 * len(train_dataset))\nval_size = len(train_dataset) - tr_size\ntr_dataset, val_dataset = random_split(train_dataset, [tr_size, val_size])\nprint('tr:', len(tr_dataset), 'valid:', len(val_dataset))\n\ntr_loader = DataLoader(tr_dataset, batch_size=32, shuffle=True, num_workers=4)\nval_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"images, labels = next(iter(tr_loader))\nprint(images.shape, labels.shape)\nprint(images[0].max(), images[0].min(), labels.min(), labels.max())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# tr_dataset은 Subset\nprint(type(tr_dataset), '\\n', type(train_dataset))\nprint(tr_dataset, '\\n', train_dataset)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# targets는 10개 target classes의 0 ~ 9까지의 값. clsses는 0~9까지의 target값에 매핑되는 label명\n# tr_dataset은 subset으로 .classes 속성이 없음. train_dataset.classes로 확인\nprint(train_dataset.classes)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataset.data","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#train_dataset[0]은 호출시 마다 ToTensor()로 변환됨. 이미지 표현을 위해 PIL이나 Numpy array 필요. \n#train_dataset.data 는 image를 numpy 배열 형태로 가짐(channel last)\nprint(type(train_dataset.data), train_dataset.data.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nclass_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n\ndef show_images(images, labels, ncols=8):\n    figure, axs = plt.subplots(figsize=(22, 6), nrows=1, ncols=ncols)\n    for i in range(ncols):\n        # imshow()는 numpy array를 그대로 이미지화 시킬 수 있음. \n        axs[i].imshow(images[i])\n        axs[i].set_title(class_names[labels[i]])\n        \nshow_images(train_dataset.data[:8], train_dataset.targets[:8], ncols=8)\nshow_images(train_dataset.data[8:16], train_dataset.targets[8:16], ncols=8)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### CNN 기반 모델 생성 - 03\n* Conv의 필터수 및 네트웍의 깊이를 좀 더 증가 시켜 모델 구성.\n* conv->relu->conv->relu->pooling을 연속적으로 수행. kernel 크기는 3, filter 수는 블럭별로 32 -> 64 -> 128로 증가. ","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchinfo import summary\n\nNUM_INPUT_CHANNELS = 3\n\nclass SimpleCNN(nn.Module):\n    def __init__(self, num_classes):\n        super().__init__()\n\n        #padding 1로 conv 적용 후 출력 면적 사이즈를 입력 면적 사이즈와 동일하게 유지.\n        #kernel 크기 3, filter 개수 32 연속 적용.\n        self.conv_11 = nn.Conv2d(in_channels=NUM_INPUT_CHANNELS, out_channels=32, kernel_size=3, stride=1, padding=1)\n        self.conv_12 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, stride=1, padding=1)\n        self.pool_01 = nn.MaxPool2d(kernel_size=2)\n        \n        #out_channels이 64인 2개의 Conv2d 연속 적용. stride=1이 기본값, padding='same'은 version 1.8에서 소개됨.  \n        self.conv_21 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding='same')\n        self.conv_22 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding='same')\n        self.pool_02 = nn.MaxPool2d(kernel_size=2)\n\n        # Sequential Module을 이용하여 Conv Layer들을 생성. 이 경우 relu activation위해 ReLU Layer 연결 생성 필요.\n        # filter갯수 128개인 Conv Layer 2개 적용 후 Max Pooling 적용.\n        self.conv_block = nn.Sequential(\n            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding='same'),\n            nn.ReLU(),\n            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding='same'),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2)\n        )\n        # GAP 및 최종 Classifier Layer\n        self.adapt_pool = nn.AdaptiveAvgPool2d(output_size=(1,1))\n        self.classifier = nn.Linear(in_features=128, out_features=num_classes)\n        # self.classifier_block = nn.Sequential(\n        #     nn.AdaptiveAvgPool2d(output_size=(1, 1)),\n        #     nn.Flatten(),\n        #     nn.Linear(in_features=128, out_features=num_classes)\n        # )\n        \n    def forward(self, x):\n        x = F.relu(self.conv_11(x))\n        x = F.relu(self.conv_12(x))\n        x = self.pool_01(x)\n        # x = F.max_pool2d(x, 2)\n\n        x = F.relu(self.conv_21(x))\n        x = F.relu(self.conv_22(x))\n        x = self.pool_02(x)\n\n        x = self.conv_block(x)\n        \n        # global pooling\n        x = self.adapt_pool(x)\n        x = torch.flatten(x, start_dim=1) #또는 x = x.view(x.size(0), -1)\n        \n        # final classification\n        x = self.classifier(x)\n        # 또는 아래와 같이 classifier_block을 forward\n        \n        return x\n\n\nsimple_cnn = SimpleCNN(num_classes=10)\n\nsummary(model=simple_cnn, input_size=(1, 3, 32, 32), \n        col_names=['input_size', 'output_size', 'num_params'], \n        row_settings=['var_names'])\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Trainer 클래스를 이용하여 학습 수행","metadata":{}},{"cell_type":"code","source":"from tqdm import tqdm\nimport torch.nn.functional as F\n\nclass Trainer:\n    def __init__(self, model, loss_fn, optimizer, train_loader, val_loader, device=None):\n        self.model = model.to(device)\n        self.loss_fn = loss_fn\n        self.optimizer = optimizer\n        self.train_loader = train_loader\n        self.val_loader = val_loader\n        self.device = device\n    \n    def train_epoch(self, epoch):\n        self.model.train()\n        \n        # running 평균 loss 계산. \n        accu_loss = 0.0\n        running_avg_loss = 0.0\n        # 정확도, 정확도 계산을 위한 전체 건수 및 누적 정확건수\n        num_total = 0.0\n        accu_num_correct = 0.0\n        accuracy = 0.0\n        \n        # tqdm으로 실시간 training loop 진행 상황 시각화\n        with tqdm(total=len(self.train_loader), desc=f\"Epoch {epoch+1} [Training..]\", leave=True) as progress_bar:\n            for batch_idx, (inputs, targets) in enumerate(self.train_loader):\n                # 반드시 to(self.device). to(device) 아님. \n                inputs = inputs.to(self.device)\n                targets = targets.to(self.device)\n                \n                # Forward pass\n                outputs = self.model(inputs)\n                loss = self.loss_fn(outputs, targets)\n                \n                # Backward pass\n                self.optimizer.zero_grad()\n                loss.backward()\n                self.optimizer.step()\n\n                # batch 반복 시 마다 누적  loss를 구하고 이를 batch 횟수로 나눠서 running 평균 loss 구함.  \n                accu_loss += loss.item()\n                running_avg_loss = accu_loss /(batch_idx + 1)\n\n                # accuracy metric 계산\n                # outputs 출력 예측 class값과 targets값 일치 건수 구하고\n                num_correct = (outputs.argmax(-1) == targets).sum().item()\n                # 배치별 누적 전체 건수와 누적 전체 num_correct 건수로 accuracy 계산\n                num_total += inputs.shape[0]\n                accu_num_correct += num_correct\n                accuracy = accu_num_correct / num_total\n\n                #tqdm progress_bar에 진행 상황 및 running 평균 loss와 정확도 표시\n                progress_bar.update(1)\n                if batch_idx % 20 == 0 or (batch_idx + 1) == progress_bar.total:  # 20 batch횟수마다 또는 맨 마지막 batch에서 update \n                    progress_bar.set_postfix({\"Loss\": running_avg_loss, \n                                              \"Accuracy\": accuracy})\n        \n        return running_avg_loss, accuracy\n                \n    def validate_epoch(self, epoch):\n        if not self.val_loader:\n            return None\n            \n        self.model.eval()\n\n        # running 평균 loss 계산. \n        accu_loss = 0\n        running_avg_loss = 0\n        # 정확도, 정확도 계산을 위한 전체 건수 및 누적 정확건수\n        num_total = 0.0\n        accu_num_correct = 0.0\n        accuracy = 0.0\n        with tqdm(total=len(self.val_loader), desc=f\"Epoch {epoch+1} [Validating]\", leave=True) as progress_bar:\n            with torch.no_grad():\n                for batch_idx, (inputs, targets) in enumerate(self.val_loader):\n                    inputs = inputs.to(self.device)\n                    targets = targets.to(self.device)\n                    \n                    outputs = self.model(inputs)\n                    \n                    loss = self.loss_fn(outputs, targets)\n                    # batch 반복 시 마다 누적  loss를 구하고 이를 batch 횟수로 나눠서 running 평균 loss 구함.  \n                    accu_loss += loss.item()\n                    running_avg_loss = accu_loss /(batch_idx + 1)\n\n                    # accuracy metric 계산\n                    # outputs 출력 예측 class값과 targets값 일치 건수 구하고\n                    num_correct = (outputs.argmax(-1) == targets).sum().item()\n                    # 배치별 누적 전체 건수와 누적 전체 num_correct 건수로 accuracy 계산  \n                    num_total += inputs.shape[0]\n                    accu_num_correct += num_correct\n                    accuracy = accu_num_correct / num_total\n\n                    #tqdm progress_bar에 진행 상황 및 running 평균 loss와 정확도 표시\n                    progress_bar.update(1)\n                    if batch_idx % 20 == 0 or (batch_idx + 1) == progress_bar.total:  # 20 batch횟수마다 또는 맨 마지막 batch에서 update \n                        progress_bar.set_postfix({\"Loss\": running_avg_loss, \n                                                  \"Accuracy\":accuracy})\n        return running_avg_loss, accuracy\n    \n    def fit(self, epochs):\n        # epoch 시마다 학습/검증 결과를 기록하는 history dict 생성.\n        history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n        for epoch in range(epochs):\n            train_loss, train_acc = self.train_epoch(epoch)\n            val_loss, val_acc = self.validate_epoch(epoch)\n            print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f} Train Accuracy: {train_acc:.4f}\",\n                  f\", Val Loss: {val_loss:.4f} Val Accuracy: {val_acc:.4f}\" if val_loss is not None else \"\")\n            # epoch 시마다 학습/검증 결과를 기록.\n            history['train_loss'].append(train_loss); history['train_acc'].append(train_acc)\n            history['val_loss'].append(val_loss); history['val_acc'].append(val_acc)\n            \n        return history \n    \n    # 학습이 완료된 모델을 return \n    def get_trained_model(self):\n        return self.model","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch \nimport torch.nn as nn\nfrom torch.optim import SGD, Adam\n\nNUM_INPUT_CHANNELS = 3\nNUM_CLASSES = 10\n\nmodel = SimpleCNN(num_classes=NUM_CLASSES)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\noptimizer = Adam(model.parameters(), lr=0.001)\nloss_fn = nn.CrossEntropyLoss()\n\ntrainer = Trainer(model=model, loss_fn=loss_fn, optimizer=optimizer,\n       train_loader=tr_loader, val_loader=val_loader, device=device)\n# 학습 및 평가 \nhistory = trainer.fit(30)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# 시각화 \ndef show_history(history, metric='acc'):\n    if metric == 'loss':\n        train_metric_name = 'train_loss'\n        val_metric_name = 'val_loss'\n    else:\n        train_metric_name = 'train_acc'\n        val_metric_name = 'val_acc'\n        \n    plt.plot(history[train_metric_name], label='train')\n    plt.plot(history[val_metric_name], label='valid')\n    plt.legend()\n    \nshow_history(history, metric='loss')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Predictor 클래스로 모델 성능 평가 및 이미지 예측","metadata":{}},{"cell_type":"code","source":"class Predictor:\n    def __init__(self, model, device):\n        self.model = model.to(device)\n        self.device = device\n\n    def evaluate(self, loader):\n        self.model.eval()\n        eval_metric = 0.0\n        \n        num_total = 0.0\n        accu_num_correct = 0.0\n\n        with tqdm(total=len(loader), desc=f\"[Evaluating]\", leave=True) as progress_bar:\n            with torch.no_grad():\n                for batch_idx, (inputs, targets) in enumerate(loader):\n                    inputs = inputs.to(self.device)\n                    targets = targets.to(self.device)\n                    pred = self.model(inputs)\n\n                    # 정확도 계산을 위해 누적 전체 건수와 누적 전체 num_correct 건수 계산  \n                    num_correct = (pred.argmax(-1) == targets).sum().item()\n                    num_total += inputs.shape[0]\n                    accu_num_correct += num_correct\n                    eval_metric = accu_num_correct / num_total\n\n                    progress_bar.update(1)\n                    if batch_idx % 20 == 0 or (batch_idx + 1) == progress_bar.total:\n                        progress_bar.set_postfix({\"Accuracy\": eval_metric})\n        \n        return eval_metric\n\n    def predict_proba(self, inputs):\n        self.model.eval()\n        with torch.no_grad():\n            inputs = inputs.to(self.device)\n            outputs = self.model(inputs)\n            #예측값을 반환하므로 targets은 필요 없음.\n            #targets = targets.to(self.device)\n            pred_proba = F.softmax(outputs, dim=-1) #또는 dim=1\n\n        return pred_proba\n\n    def predict(self, inputs):\n        pred_proba = self.predict_proba(inputs)\n        pred_class = torch.argmax(pred_proba, dim=-1)\n\n        return pred_class","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trained_model = trainer.get_trained_model()\n\n# 학습데이터와 동일하게 정규화된 데이터를 입력해야 함. \n# test_dataset = CIFAR10(root='./data', train=False, download=True, transform=ToTensor())\n# test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)\n\npredictor = Predictor(model=trained_model, device=device)\neval_metric = predictor.evaluate(test_loader)\nprint(f'test dataset evaluation:{eval_metric:.4f}')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\nplt.figure(figsize=(1, 1))\nplt.imshow(test_dataset.data[0])\nplt.title(class_names[test_dataset.targets[0]])\n\nprint('target value:', test_dataset.targets[0])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_dataset[0]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 반드시 예측할 이미지는 tensor로, shape는 4차원으로 입력. 이를 위해 unsqueeze(0)\npred_class = predictor.predict(test_dataset[0][0].unsqueeze(0))\nprint('predicted class:', pred_class.item())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 출력 Feature Map의 면적 계산하기\n* 입력 Feature Map의 면적과 Convolution 적용 Kernel size, stride 및 padding에 따른 출력 Feature Map의 면적 계산\n* I는 입력 Feature Map의 면적(크기), K는 Filter의 Kernel size, P는 Padding(정수), S는 Strides(정수)\n* O = (I - K + 2P)/S + 1 ","metadata":{}},{"cell_type":"markdown","source":"#### Stride가 1이고 Padding이 없는 경우 - Kernel size 3 적용\n* I는 입력 Feature Map의 크기, K는 Filter의 Kernel size, P는 Padding(정수), S는 Strides(정수)\n* O = (I - K + 2P)/1 + 1 = (5 - 3 + 0 )/1 + 1 = 3","metadata":{}},{"cell_type":"code","source":"import torch.nn as nn\nimport torch\n\ninput = torch.randn(1, 5, 5)\nconv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, stride=1, padding=0) #kernel_size=5로 적용\noutput = conv1(input)\nprint(output.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Stride가 1이고 Padding이 1인 경우\n* O = (I - F + 2P)/S + 1 = (5 - 3 + 2 )/1 + 1 = 5","metadata":{}},{"cell_type":"code","source":"input = torch.randn(1, 5, 5)\nconv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, padding=1) #padding='same'으로 적용 \noutput = conv1(input)\nprint(output.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Zero Padding 이 적용된 Output 보기\ninput = torch.randn(1, 5, 5)\npadding_layer = nn.ZeroPad2d(padding=1)\npadded_input = padding_layer(input)\nprint('padded input shape:', padded_input.shape)\n\nconv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3)\noutput = conv1(padded_input)\nprint(output.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Padding 시 상하 좌우가 다른 값을 넣기\n* Conv2d의 padding값으로 Tuple을 입력. 예를 들어 padding=(1, 2)이면 상하가 1, 좌우가 2임.\n* 상하가 다른값, 또는 좌우가 다른 값으로 Padding 적용하려면 nn.ZeroPad2d나 F.pad()함수를 사용해야 함.","metadata":{}},{"cell_type":"code","source":"input = torch.randn(1, 5, 5)\nconv1 = nn.Conv2d(in_channels=1, out_channels=16, \n                  kernel_size=3, padding=(1, 2)) # padding=Tuple에서 맨앞이 상하, 뒤가 좌우\noutput = conv1(input)\nprint(output.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Zero Padding 이 적용된 Output 보기\ninput = torch.randn(1, 5, 5)\n#padding = (left, right, top, bottom)로 좌,우,상,하 순서임.\npadding_layer = nn.ZeroPad2d(padding=(0, 1, 0, 2))\npadded_input = padding_layer(input)\nprint('padded input shape:', padded_input.shape)\n\nconv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3)\noutput = conv1(padded_input)\nprint(output.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch.nn.functional as F\n\ninput = torch.randn(1, 5, 5)\n#pad = (left, right, top, bottom)로 좌,우,상,하 순서임.\npadded_input = F.pad(input, pad=(0, 1, 0, 2), mode='constant', value=0)\nprint(padded_input.shape)\n\nconv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3)\noutput = conv1(padded_input)\nprint(output.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Stride가 2이고 Padding이 없는 경우 - Kernel size 3 적용\n* O = (I - K + 2P)/S + 1 = (5 - 3)/2 + 1 = 2","metadata":{}},{"cell_type":"code","source":"input = torch.randn(1, 5, 5)\nconv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, stride=2)\noutput = conv1(input)\nprint(output.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Conv의 stride가 2이상일 경우, padding='same'은 적용할 수 없음. ","metadata":{}},{"cell_type":"code","source":"input = torch.randn(1, 5, 5)\nconv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, stride=2, padding='same')\noutput = conv1(input)\nprint(output.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"input = torch.randn(1, 5, 5)\nconv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, stride=2, padding=(1, 1)) \noutput = conv1(input)\nprint(output.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 입력이 6X6에서 Kernel_size=3, Stride=2 적용\n* O = (I - K + 2P)/2 + 1 = (6 - 3 + 0)/2 + 1 = 2.5 = 2\n* stride=2 적용 시에는 맨 가장자리를 Convolution 적용하지 못하는 경우를 피하기 위해 일반적으로 padding 더해줌","metadata":{}},{"cell_type":"code","source":"input = torch.randn(1, 6, 6)\nconv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, stride=2) \noutput = conv1(input)\nprint(output.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"input = torch.randn(1, 6, 6)\nconv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, stride=2, padding=(1, 1)) \noutput = conv1(input)\nprint(output.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Maxpooling 적용","metadata":{}},{"cell_type":"code","source":"input = torch.randn(1, 224, 224) # (1, 223, 223)\nmax_pool = nn.MaxPool2d(kernel_size=2, stride=2) \noutput = max_pool(input)\nprint(output.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}