{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9519275,"sourceType":"datasetVersion","datasetId":5795628}],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### VGG 16 모델 살펴 보기","metadata":{}},{"cell_type":"code","source":"from torchvision import models\n\nprint(models.list_models(include='vgg*'))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"torch_vgg16_bn_model = models.vgg16_bn(weights=None)\nprint(torch_vgg16_bn_model)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### VGG 16 모델 생성 - 01","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nIMG_CHANNELS = 3\n\nclass VGG16_01(nn.Module):\n    def __init__(self, num_classes=10):\n        super().__init__()\n\n        self.conv_block_01 = nn.Sequential(\n            nn.Conv2d(in_channels=IMG_CHANNELS, out_channels=64, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(num_features=64), nn.ReLU(),\n            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(num_features=64), nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2)\n        )\n\n        self.conv_block_02 = nn.Sequential(\n            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(num_features=128), nn.ReLU(),\n            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(num_features=128), nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2)\n        )\n\n        self.conv_block_03 = nn.Sequential(\n            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(num_features=256), nn.ReLU(),\n            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(num_features=256), nn.ReLU(),\n            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(num_features=256), nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2)\n        )\n\n        self.conv_block_04 = nn.Sequential(\n            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(num_features=512), nn.ReLU(),\n            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(num_features=512), nn.ReLU(),\n            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(num_features=512), nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2)\n        )\n\n        self.conv_block_05 = nn.Sequential(\n            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(num_features=512), nn.ReLU(),\n            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(num_features=512), nn.ReLU(),\n            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(num_features=512), nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2)\n        )\n\n        self.adaptive_pool = nn.AdaptiveAvgPool2d(output_size=(7, 7))\n        self.classifier = nn.Sequential(\n            nn.Linear(in_features=25088, out_features=4096, bias=True),\n            nn.ReLU(inplace=True),\n            nn.Dropout(p=0.5, inplace=False), \n            nn.Linear(in_features=4096, out_features=4096, bias=True),\n            nn.ReLU(inplace=True),\n            nn.Dropout(p=0.5, inplace=False),\n            nn.Linear(in_features=4096, out_features=num_classes, bias=True)\n        )\n\n    def forward(self, x):\n        x = self.conv_block_01(x)\n        x = self.conv_block_02(x)\n        x = self.conv_block_03(x)\n        x = self.conv_block_04(x)\n        x = self.conv_block_05(x)\n\n        x = self.adaptive_pool(x)\n        x = torch.flatten(x, start_dim=1) # x.view(x.size(0), -1)\n        x = self.classifier(x)\n\n        return x","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torchinfo import summary\n\nmy_vgg16_01 = VGG16_01(num_classes=1000)\n\nsummary(model=my_vgg16_01, input_size=(1, 3, 224, 224),\n        col_names=['input_size', 'output_size', 'num_params'], \n        row_settings=['var_names'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torchinfo import summary\n\ntorch_vgg16 = models.vgg16_bn(weights='DEFAULT')\n\nsummary(model=torch_vgg16, input_size=(1, 3, 224, 224),\n        col_names=['input_size', 'output_size', 'num_params'], \n        row_settings=['var_names'])\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 연속되는 Conv 적용 Sequential을 생성하는 함수를 생성하여 VGG16 모델 구현\n* create_convbn_block(in_channels, last_channels, num_convs)는 num_convs 만큼 반복하여 in_channels과 last_channels를 Conv 적용한 Sequential로 반환","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nIMG_CHANNELS = 3\n\nclass VGG16_02(nn.Module):\n    def __init__(self, num_classes=10):\n        super().__init__()\n        # Feature Extractor\n        self.conv_block_01 = self.create_convbn_block(in_channels=3, last_channels=64, num_convs=2)\n        self.conv_block_02 = self.create_convbn_block(64, 128, num_convs=2)\n        self.conv_block_03 = self.create_convbn_block(128, 256, num_convs=3)\n        self.conv_block_04 = self.create_convbn_block(256, 512, num_convs=3)\n        self.conv_block_05 = self.create_convbn_block(512, 512, num_convs=3)\n\n        #GAP와 Classifier Layer\n        self.adaptive_pool = nn.AdaptiveAvgPool2d(output_size=(7, 7))\n        self.classifier = nn.Sequential(\n            nn.Linear(in_features=25088, out_features=4096, bias=True),\n            nn.ReLU(inplace=True),\n            nn.Dropout(p=0.5, inplace=False),\n            nn.Linear(in_features=4096, out_features=4096, bias=True),\n            nn.ReLU(inplace=True),\n            nn.Dropout(p=0.5, inplace=False),\n            nn.Linear(in_features=4096, out_features=num_classes, bias=True)\n        )\n\n    def create_convbn_block(self, in_channels, last_channels, num_convs=2):\n        if num_convs == 2:\n            conv_bn_block = nn.Sequential(\n                nn.Conv2d(in_channels=in_channels, out_channels=last_channels, kernel_size=3, padding=1),\n                nn.BatchNorm2d(last_channels), nn.ReLU(),\n                nn.Conv2d(in_channels=last_channels, out_channels=last_channels, kernel_size=3, padding=1),\n                nn.BatchNorm2d(last_channels), nn.ReLU(),\n                nn.MaxPool2d(kernel_size=2)\n            )\n        elif num_convs == 3:\n            conv_bn_block = nn.Sequential(\n                nn.Conv2d(in_channels=in_channels, out_channels=last_channels, kernel_size=3, padding=1),\n                nn.BatchNorm2d(last_channels), nn.ReLU(),\n                nn.Conv2d(in_channels=last_channels, out_channels=last_channels, kernel_size=3, padding=1),\n                nn.BatchNorm2d(last_channels), nn.ReLU(),\n                nn.Conv2d(in_channels=last_channels, out_channels=last_channels, kernel_size=3, padding=1),\n                nn.BatchNorm2d(last_channels), nn.ReLU(),\n                nn.MaxPool2d(kernel_size=2)\n            )\n        \n        return conv_bn_block\n\n    def forward(self, x):\n        # Feature Extractor forward\n        x = self.conv_block_01(x)\n        x = self.conv_block_02(x)\n        x = self.conv_block_03(x)\n        x = self.conv_block_04(x)\n        x = self.conv_block_05(x)\n\n        # GAP, 마지막 Classifier forward\n        x = self.adaptive_pool(x)\n        x = torch.flatten(x, start_dim=1) #x.view(x.size(0), -1)\n        x = self.classifier(x)\n\n        return x","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"my_vgg16_02 = VGG16_02(num_classes=1000)\n\nsummary(model=my_vgg16_02, input_size=(1, 3, 224, 224),\n        col_names=['input_size', 'output_size', 'num_params'], \n        row_settings=['var_names'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 연속되는 Conv block을 가지는 VGGBlock 서브 모듈을 만들고 이를 활용하여 VGG16 모델 구현\n* ConvBlock은 Conv->BN->ReLU 구성\n* VGGBlock은 연속되는 ConvBlock을 list에 담고 Sequential로 연결함","metadata":{}},{"cell_type":"code","source":"class ConvBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size=3, padding=1):\n        super().__init__()\n        self.block = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size, padding=padding),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU()\n        )\n    def forward(self, x):\n        return self.block(x)\n\nclass VGGBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, num_convs):\n        super().__init__()\n        # num_convs 값에 따라 ConvBlock을 담아둘 list\n        layers = []\n        for i in range(num_convs):\n            layers.append(ConvBlock(in_channels, out_channels))\n            in_channels = out_channels\n        layers.append(nn.MaxPool2d(kernel_size=2))\n        # layers list에 있는 모든 Layer들을 Sequential로 담아서 연결\n        self.block = nn.Sequential(*layers)\n\n    def forward(self, x):\n        return self.block(x)\n\nclass VGG16(nn.Module):\n    def __init__(self, num_classes=10):\n        super().__init__()\n        self.features = nn.Sequential(\n            VGGBlock(3, 64, num_convs=2),\n            VGGBlock(64, 128, num_convs=2),\n            VGGBlock(128, 256, num_convs=3),\n            VGGBlock(256, 512, num_convs=3),\n            VGGBlock(512, 512, num_convs=3),\n        )\n        #GAP와 Classifier Layer\n        self.adaptive_pool = nn.AdaptiveAvgPool2d(output_size=(7, 7))\n        self.classifier = nn.Sequential(\n            nn.Linear(in_features=25088, out_features=4096, bias=True),\n            nn.ReLU(inplace=True),\n            nn.Dropout(p=0.5, inplace=False),\n            nn.Linear(in_features=4096, out_features=4096, bias=True),\n            nn.ReLU(inplace=True),\n            nn.Dropout(p=0.5, inplace=False),\n            nn.Linear(in_features=4096, out_features=num_classes, bias=True)\n        )\n\n    def forward(self, x):\n        x = self.features(x)\n        x = self.adaptive_pool(x)\n        x = torch.flatten(x, start_dim=1)\n        x = self.classifier(x)\n        return x\n\n    ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"vgg16_model = VGG16(num_classes=1000)\nprint(vgg16_model)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torchinfo import summary \n\nsummary(model=vgg16_model, input_size=(1, 3, 224, 224),\n        col_names=['input_size', 'output_size', 'num_params'], \n        depth=4, #depth=3 이 기본. 이 경우 ConvBlock의 내부 Layer가 보이지 않음\n        row_settings=['var_names'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### VGG16 모델 학습 및 평가\n* Flowers 데이터 세트로 학습 및 평가\n* Trainer 클래스는 Modular 활용","metadata":{}},{"cell_type":"code","source":"!ls /kaggle/input","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\n\ndef create_flowers_meta_df(file_dir):\n    paths = [] # 이미지 파일 경로 리스트\n    labels = [] # 꽃 종류\n    \n    # os.walk()를 이용하여 특정 디렉토리 밑에 있는 모든 하위 디렉토리를 모두 조사. \n    # kaggle/input/flowers-dataset 하위 디렉토리 밑에 jpg 확장자를 가진 파일이 모두 이미지 파일임\n    # kaggle/input/flowers-dataset 밑으로 하위 디렉토리 존재\n    for dirname, _, filenames in os.walk(file_dir):\n        for filename in filenames:\n            # 이미지 파일이 아닌 파일도 해당 디렉토리에 있음.\n            if '.jpg' in filename:\n                # 파일의 절대 경로를 file_path 변수에 할당. \n                file_path = dirname+'/'+ filename\n                paths.append(file_path)\n                \n                # 파일의 절대 경로에 daily, dandelion, roses, sunflowers, tulips에 따라 labels에 값 할당.               label_gubuns.append('daisy')\n                if 'daisy' in file_path:\n                    labels.append('daisy')\n                elif 'dandelion' in file_path:\n                    labels.append('dandelion')\n                elif 'roses' in file_path:\n                    labels.append('rose')\n                elif 'sunflowers' in file_path:\n                    labels.append('sunflowers')\n                elif 'tulips' in file_path:\n                    labels.append('tulips')\n    # DataFrame 메타 데이터 생성. \n    data_df = pd.DataFrame({'path':paths, \n                            'label':labels})\n    # Target값  변환\n    label_mapping = {'daisy': 0, 'dandelion': 1, 'rose': 2, 'sunflowers': 3, 'tulips': 4}\n    data_df['target'] = data_df['label'].map(label_mapping)\n\n    return data_df","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ndata_df = create_flowers_meta_df('/kaggle/input/flowers-dataset') # /kaggle/input\n\n# 전체 데이터 세트에서 학습(전체의 70%)과 테스트용(전체의 30%) 메타 정보 DataFrame 생성.\ntrain_df, test_df = train_test_split(data_df, test_size=0.3, stratify=data_df['target'], random_state=2025)\n# 기존 학습 DataFrame을 다시 학습과 검증 DataFrame으로 분할. 80%가 학습, 20%가 검증\ntr_df, val_df = train_test_split(train_df, test_size=0.2, stratify=train_df['target'], random_state=2025)\n\nprint(data_df.shape, train_df.shape, tr_df.shape, val_df.shape, test_df.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms as T\nfrom PIL import Image\n\nBATCH_SIZE = 16\n\nclass FlowerDataset(Dataset):\n    # 이미지 파일리스트, 타겟 파일리스트, transforms 등 이미지와 타겟 데이터 가공에 필요한 인자들을 입력 받음\n    def __init__(self, image_paths, targets=None, transform=None):\n        self.image_paths = image_paths\n        self.targets = targets\n        self.transform = transform\n    \n    # 전체 건수를 반환\n    def __len__(self):\n        return len(self.image_paths)\n        \n    # idx로 지정된 하나의 image, label을 tensor 형태로 반환\n    def __getitem__(self, idx):    \n        # PIL을 이용하여 이미지 로딩하고 PIL Image 객체 반환.\n        pil_image = Image.open(self.image_paths[idx])\n        # 보통은 transform이 None이 되는 경우는 거의 없음(Tensor 변환이라도 있음)\n        image = self.transform(pil_image)\n\n        if self.targets is not None:\n            # 개별 target값을 tensor로 변환.\n            target = torch.tensor(self.targets[idx])\n            return image, target\n        # 테스트 데이터의 경우 targets가 입력 되지 않을 수 있으므로 이를 대비. \n        else:\n            return image\n\ndef create_tr_val_loader(tr_df, val_df, tr_transform, val_transform):\n    tr_dataset = FlowerDataset(image_paths=tr_df['path'].to_list(), \n                               targets=tr_df['target'].to_list(), transform=tr_transform)\n    val_dataset = FlowerDataset(image_paths=val_df['path'].to_list(), \n                                targets=val_df['target'].to_list(), transform=val_transform)\n    \n    tr_loader = DataLoader(tr_dataset, batch_size = BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)\n    val_loader = DataLoader(val_dataset, batch_size=2*BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)\n\n    return tr_loader, val_loader","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"IMG_SIZE = 224\nIMG_MEANS = [0.485, 0.456, 0.406] # ImageNet 데이터세트의 이미지 채널별 평균값\nIMG_STD = [0.229, 0.224, 0.225] # ImageNet 데이터세트의 이미지 채널별 표준편차값\n\ntr_transform = T.Compose([\n            T.RandomHorizontalFlip(p=0.3),\n            T.RandomVerticalFlip(p=0.3),\n            T.RandomApply([T.CenterCrop(size=(200, 200))], p=0.4),\n            T.Resize(size=(IMG_SIZE, IMG_SIZE)),\n            T.ToTensor(), T.Normalize(mean=IMG_MEANS, std=IMG_STD)\n])\n\nval_transform = T.Compose([\n            T.Resize(size=(IMG_SIZE, IMG_SIZE)),\n            T.ToTensor(), T.Normalize(mean=IMG_MEANS, std=IMG_STD)\n])\n\ntr_loader, val_loader = create_tr_val_loader(tr_df=tr_df, val_df=val_df, \n                                             tr_transform=tr_transform, val_transform=val_transform)\nimages, labels = next(iter(tr_loader))\nprint(images.shape, labels.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Trainer 클래스 생성 및 적용\n* Trainer 클래스는 https://raw.githubusercontent.com/chulminkw/CNN_PG_Torch/main/modular/v1/utils.py?raw=true 로 download 후 import 함","metadata":{}},{"cell_type":"code","source":"# /kaggle/working/modular/v1 디렉토리에 utils.py 파일 다운로드\n!rm -rf ./modular/v1\n!mkdir -p ./modular/v1\n!wget -O ./modular/v1/utils.py https://raw.githubusercontent.com/chulminkw/CNN_PG_Torch/main/modular/v1/utils.py?raw=true\n!ls ./modular/v1\n\nimport sys\n\n# 반드시 system path를 아래와 같이 잡아줘야 함. \nsys.path.append('/kaggle/working')\n\n#아래가 수행되는지 반드시 확인\nfrom modular.v1.utils import Trainer, Predictor, ModelCheckpoint, EarlyStopping","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.optim import SGD, Adam\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\n\n# 5개의 꽃 종류\nNUM_CLASSES = 5\n\n# 직접 구현한 VGG16 모델 생성\nmodel = VGG16(num_classes=NUM_CLASSES)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\noptimizer = Adam(model.parameters(), lr=0.0001) #lr = 1e-4\nloss_fn = nn.CrossEntropyLoss()\nscheduler = ReduceLROnPlateau(\n            optimizer=optimizer, mode='min', factor=0.5, patience=5, threshold=0.01, min_lr=1e-6)\n\ntrainer = Trainer(model=model, loss_fn=loss_fn, optimizer=optimizer,\n       train_loader=tr_loader, val_loader=val_loader, scheduler=scheduler, device=device)\n# 학습 및 평가.\nhistory = trainer.fit(60)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from modular.v1.utils import Predictor\n\ntest_image_paths = test_df['path'].to_list()\ntest_targets = test_df['target'].to_list()\n\nIMG_SIZE=224\ntest_transform = T.Compose([\n                        T.Resize(size=(IMG_SIZE, IMG_SIZE)),\n                        T.ToTensor(), \n                        T.Normalize(mean=[0.485, 0.456, 0.406], \n                                    std=[0.229, 0.224, 0.225])\n])\n\ntest_dataset = FlowerDataset(image_paths=test_image_paths, \n                            targets=test_targets, transform=test_transform)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4, pin_memory=True)\n\ntrained_model = trainer.get_trained_model()\n\npredictor = Predictor(model=trained_model, device=device)\neval_metric = predictor.evaluate(test_loader)\nprint(f'test dataset evaluation:{eval_metric:.4f}')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}