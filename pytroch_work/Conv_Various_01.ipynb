{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### 가중치 초기화\n* pytorch의 nn.Linear와 nn.Conv2d는 He Uniform 기반 weight 초기화\n* 가중치 초기화는 nn.init.kaiming_uniform_(), nn.init.kaiming_normal_()등을 이용","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport math\n\ntorch.manual_seed(2025)\n\nlinear_01 = nn.Linear(in_features=12, out_features=6)\nprint(f'weight boundary: {linear_01.weight.min().item()} ~ {linear_01.weight.max().item()}')\n\n# pytorch weight는 1/sqrt(fan_in)\nfan_in = linear_01.in_features  \nbound = 1 / math.sqrt(fan_in)\nprint('bound:', bound)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"torch.manual_seed(2025)\n\nconv_01 = nn.Conv2d(in_channels=12, out_channels=24, kernel_size=3)\nprint(f'weight boundary: {conv_01.weight.min().item()} ~ {conv_01.weight.max().item()}')\n\n# pytorch weight는 1/sqrt(fan_in). Conv2d의 fan_in은 in_channels * kernel_height * kernel_width\nfan_in = conv_01.in_channels * 3 * 3\nbound = 1 / math.sqrt(fan_in)\nprint('bound:', bound)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class SimpleCNN_01(nn.Module):\n    def __init__(self, num_classes=10):\n        super().__init__()\n        self.conv_1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=1)\n        self.conv_2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1)\n        self.pool = nn.MaxPool2d(kernel_size=2)\n        self.flatten = nn.Flatten()\n        self.classifier = nn.Linear(in_features=12544, out_features=num_classes)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                print('kaiming normal initialization applied')\n                nn.init.kaiming_normal_(m.weight, mode=\"fan_in\", nonlinearity=\"relu\")\n\n    def forward(self, x):\n        x = F.relu(self.conv_1(x))\n        x = F.relu(self.conv_2(x))\n        x = self.pool(x)\n        x = self.flatten(x)\n        x = self.classifier(x)\n\n        return x","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"simple_cnn_01 = SimpleCNN_01(num_classes=10)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Batch Normalization 적용\n* Linear Layer 이후 적용 시는 BatchNorm1d(num_features) 를 적용. num_features는 Linear Layer의 out_features와 동일\n* Conv2d Layer 이후 적용 시는 BatchNorm2d(num_features) 를 적용. num_features는 Conv2d의 out_channels와 동일\n* 기존 Network 모델의 Conv -> Activation을 Conv -> BN -> Activation 으로 적용","metadata":{}},{"cell_type":"markdown","source":"#### CIFAR 10 Dataset 및 DataLoader 생성, Trainer 클래스 생성","metadata":{}},{"cell_type":"code","source":"from torchvision.datasets import CIFAR10\nfrom torchvision.transforms import ToTensor\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data import random_split\n\n#전체 6만개 데이터 중, 5만개는 학습 데이터용. 이를 다시 학습과 검증용으로 split , 1만개는 테스트 데이터용\ntrain_dataset = CIFAR10(root='./data', train=True, download=True, transform=ToTensor())\ntest_dataset = CIFAR10(root='./data', train=False, download=True, transform=ToTensor())\n\ntr_size = int(0.85 * len(train_dataset))\nval_size = len(train_dataset) - tr_size\ntr_dataset, val_dataset = random_split(train_dataset, [tr_size, val_size])\nprint('tr:', len(tr_dataset), 'valid:', len(val_dataset))\n\ntr_loader = DataLoader(tr_dataset, batch_size=32, shuffle=True, num_workers=4)\nval_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tqdm import tqdm\nimport torch.nn.functional as F\n\nclass Trainer:\n    def __init__(self, model, loss_fn, optimizer, train_loader, val_loader, device=None):\n        self.model = model.to(device)\n        self.loss_fn = loss_fn\n        self.optimizer = optimizer\n        self.train_loader = train_loader\n        self.val_loader = val_loader\n        self.device = device\n    \n    def train_epoch(self, epoch):\n        self.model.train()\n        \n        # running 평균 loss 계산. \n        accu_loss = 0.0\n        running_avg_loss = 0.0\n        # 정확도, 정확도 계산을 위한 전체 건수 및 누적 정확건수\n        num_total = 0.0\n        accu_num_correct = 0.0\n        accuracy = 0.0\n        \n        # tqdm으로 실시간 training loop 진행 상황 시각화\n        with tqdm(total=len(self.train_loader), desc=f\"Epoch {epoch+1} [Training..]\", leave=True) as progress_bar:\n            for batch_idx, (inputs, targets) in enumerate(self.train_loader):\n                # 반드시 to(self.device). to(device) 아님. \n                inputs = inputs.to(self.device)\n                targets = targets.to(self.device)\n                \n                # Forward pass\n                outputs = self.model(inputs)\n                loss = self.loss_fn(outputs, targets)\n                \n                # Backward pass\n                self.optimizer.zero_grad()\n                loss.backward()\n                self.optimizer.step()\n\n                # batch 반복 시 마다 누적  loss를 구하고 이를 batch 횟수로 나눠서 running 평균 loss 구함.  \n                accu_loss += loss.item()\n                running_avg_loss = accu_loss /(batch_idx + 1)\n\n                # accuracy metric 계산\n                # outputs 출력 예측 class값과 targets값 일치 건수 구하고\n                num_correct = (outputs.argmax(-1) == targets).sum().item()\n                # 배치별 누적 전체 건수와 누적 전체 num_correct 건수로 accuracy 계산\n                num_total += inputs.shape[0]\n                accu_num_correct += num_correct\n                accuracy = accu_num_correct / num_total\n\n                #tqdm progress_bar에 진행 상황 및 running 평균 loss와 정확도 표시\n                progress_bar.update(1)\n                if batch_idx % 20 == 0 or (batch_idx + 1) == progress_bar.total:  # 20 batch횟수마다 또는 맨 마지막 batch에서 update \n                    progress_bar.set_postfix({\"Loss\": running_avg_loss, \n                                              \"Accuracy\": accuracy})\n        \n        return running_avg_loss, accuracy\n                \n    def validate_epoch(self, epoch):\n        if not self.val_loader:\n            return None\n            \n        self.model.eval()\n\n        # running 평균 loss 계산. \n        accu_loss = 0\n        running_avg_loss = 0\n        # 정확도, 정확도 계산을 위한 전체 건수 및 누적 정확건수\n        num_total = 0.0\n        accu_num_correct = 0.0\n        accuracy = 0.0\n        with tqdm(total=len(self.val_loader), desc=f\"Epoch {epoch+1} [Validating]\", leave=True) as progress_bar:\n            with torch.no_grad():\n                for batch_idx, (inputs, targets) in enumerate(self.val_loader):\n                    inputs = inputs.to(self.device)\n                    targets = targets.to(self.device)\n                    \n                    outputs = self.model(inputs)\n                    \n                    loss = self.loss_fn(outputs, targets)\n                    # batch 반복 시 마다 누적  loss를 구하고 이를 batch 횟수로 나눠서 running 평균 loss 구함.  \n                    accu_loss += loss.item()\n                    running_avg_loss = accu_loss /(batch_idx + 1)\n\n                    # accuracy metric 계산\n                    # outputs 출력 예측 class값과 targets값 일치 건수 구하고\n                    num_correct = (outputs.argmax(-1) == targets).sum().item()\n                    # 배치별 누적 전체 건수와 누적 전체 num_correct 건수로 accuracy 계산  \n                    num_total += inputs.shape[0]\n                    accu_num_correct += num_correct\n                    accuracy = accu_num_correct / num_total\n\n                    #tqdm progress_bar에 진행 상황 및 running 평균 loss와 정확도 표시\n                    progress_bar.update(1)\n                    if batch_idx % 20 == 0 or (batch_idx + 1) == progress_bar.total:  # 20 batch횟수마다 또는 맨 마지막 batch에서 update \n                        progress_bar.set_postfix({\"Loss\": running_avg_loss, \n                                                  \"Accuracy\":accuracy})\n        return running_avg_loss, accuracy\n    \n    def fit(self, epochs):\n        # epoch 시마다 학습/검증 결과를 기록하는 history dict 생성.\n        history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n        for epoch in range(epochs):\n            train_loss, train_acc = self.train_epoch(epoch)\n            val_loss, val_acc = self.validate_epoch(epoch)\n            print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f} Train Accuracy: {train_acc:.4f}\",\n                  f\", Val Loss: {val_loss:.4f} Val Accuracy: {val_acc:.4f}\" if val_loss is not None else \"\")\n            # epoch 시마다 학습/검증 결과를 기록.\n            history['train_loss'].append(train_loss); history['train_acc'].append(train_acc)\n            history['val_loss'].append(val_loss); history['val_acc'].append(val_acc)\n            \n        return history \n    \n    # 학습이 완료된 모델을 return \n    def get_trained_model(self):\n        return self.model","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Batch Normalization을 모델에 적용 후 성능 검증\n* 기존 Network 모델의 Conv -> Activation을 Conv -> BN -> Activation 으로 적용","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchinfo import summary\n\nNUM_INPUT_CHANNELS = 3\n\nclass SimpleCNNWithBN(nn.Module):\n    def __init__(self, num_classes):\n        super().__init__()\n\n        #kernel 크기 3, filter 개수 32 연속 적용.\n        self.conv_block_1 = nn.Sequential(\n            nn.Conv2d(in_channels=NUM_INPUT_CHANNELS, out_channels=32, kernel_size=3, padding=1),\n            nn.BatchNorm2d(num_features=32),\n            nn.ReLU(),\n            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, padding=1),\n            nn.BatchNorm2d(num_features=32),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2)     \n        )\n        \n        #out_channels이 64인 2개의 Conv2d. stride=1이 기본값, padding='same'은 version 1.8에서 소개됨.  \n        self.conv_block_2 = nn.Sequential(\n            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding='same'),\n            nn.BatchNorm2d(64),\n            nn.ReLU(),\n            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding='same'),\n            nn.BatchNorm2d(64),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2)\n        )\n\n        # Sequential Module을 이용하여 Conv Layer들을 생성. 이 경우 relu activation위해 ReLU Layer 연결 생성 필요.\n        # filter갯수 128개인 Conv Layer 2개 적용 후 Max Pooling 적용. \n        self.conv_block_3 = nn.Sequential(\n            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n            nn.BatchNorm2d(128),\n            nn.ReLU(),\n            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1),\n            nn.BatchNorm2d(128),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2)\n        )\n        # GAP 및 최종 Classifier Layer\n        self.classifier_block = nn.Sequential(\n            nn.AdaptiveAvgPool2d(output_size=(1, 1)),\n            nn.Flatten(),\n            nn.Linear(in_features=128, out_features=num_classes)\n        )\n        \n    def forward(self, x):\n        x = self.conv_block_1(x)\n        x = self.conv_block_2(x)\n        x = self.conv_block_3(x)\n        x = self.classifier_block(x)\n\n        return x\n\n\nsimple_cnn = SimpleCNNWithBN(num_classes=10)\n\nsummary(model=simple_cnn, input_size=(1, 3, 32, 32), \n        col_names=['input_size', 'output_size', 'num_params'], \n        row_settings=['var_names'])\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### 반복되는 Sequential Container 부분을 함수화 적용","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchinfo import summary\n\nNUM_INPUT_CHANNELS = 3\n\nclass SimpleCNNWithBN(nn.Module):\n    def __init__(self, num_classes):\n        super().__init__()\n\n        #padding 1로 conv 적용 후 출력 면적 사이즈를 입력 면적 사이즈와 동일하게 유지. \n        self.conv_block_1 = self.create_convbn_block(first_channels=3, middle_channels=32, last_channels=32)        \n        \n        #out_channels이 64인 2개의 Conv2d. stride=1이 기본값, padding='same'은 version 1.8에서 소개됨.  \n        self.conv_block_2 = self.create_convbn_block(first_channels=32, middle_channels=64, last_channels=64)\n        \n        # filter갯수 128개인 Conv Layer 2개 적용 후 Max Pooling 적용. \n        self.conv_block_3 = self.create_convbn_block(first_channels=64, middle_channels=128, last_channels=128)\n        \n        # GAP 및 최종 Classifier Layer\n        self.classifier_block = nn.Sequential(\n            nn.AdaptiveAvgPool2d(output_size=(1, 1)),\n            nn.Flatten(),\n            nn.Linear(in_features=128, out_features=num_classes)\n        )\n\n    def create_convbn_block(self, first_channels, middle_channels, last_channels):\n        conv_bn_block = nn.Sequential(\n            nn.Conv2d(in_channels=first_channels, out_channels=middle_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(middle_channels),\n            nn.ReLU(),\n            nn.Conv2d(in_channels=middle_channels, out_channels=last_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(last_channels),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2)\n        )\n        return conv_bn_block\n        \n    def forward(self, x):\n        x = self.conv_block_1(x)\n        x = self.conv_block_2(x)\n        x = self.conv_block_3(x)\n        x = self.classifier_block(x)\n\n        return x\n\nsimple_cnn = SimpleCNNWithBN(num_classes=10)\n\nsummary(model=simple_cnn, input_size=(1, 3, 32, 32), \n        col_names=['input_size', 'output_size', 'num_params'], \n        row_settings=['var_names'])\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch \nimport torch.nn as nn\nfrom torch.optim import SGD, Adam\n\nNUM_INPUT_CHANNELS = 3\nNUM_CLASSES = 10\n\nmodel = SimpleCNNWithBN(num_classes=NUM_CLASSES)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\noptimizer = Adam(model.parameters(), lr=0.001)\nloss_fn = nn.CrossEntropyLoss()\n\ntrainer = Trainer(model=model, loss_fn=loss_fn, optimizer=optimizer,\n       train_loader=tr_loader, val_loader=val_loader, device=device)\n# 학습 및 평가 \nhistory = trainer.fit(30)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Predictor 클래스로 예측 적용","metadata":{}},{"cell_type":"code","source":"class Predictor:\n    def __init__(self, model, device):\n        self.model = model.to(device)\n        self.device = device\n\n    def evaluate(self, loader):\n        # 현재 입력으로 들어온 데이터의 batch 통계(mean, variance)를 사용하지 않고, 학습 시 계산된 running 통계값을 사용\n        self.model.eval()\n        eval_metric = 0.0\n        # 정확도 계산을 위한 전체 건수 및 누적 정확건수\n        num_total = 0.0\n        accu_num_correct = 0.0\n\n        with tqdm(total=len(loader), desc=f\"[Evaluating]\", leave=True) as progress_bar:\n            with torch.no_grad():\n                for batch_idx, (inputs, targets) in enumerate(loader):\n                    inputs = inputs.to(self.device)\n                    targets = targets.to(self.device)\n                    pred = self.model(inputs)\n\n                    # 정확도 계산을 위해 누적 전체 건수와 누적 전체 num_correct 건수 계산  \n                    num_correct = (pred.argmax(-1) == targets).sum().item()\n                    num_total += inputs.shape[0]\n                    accu_num_correct += num_correct\n                    eval_metric = accu_num_correct / num_total\n\n                    progress_bar.update(1)\n                    if batch_idx % 20 == 0 or (batch_idx + 1) == progress_bar.total:\n                        progress_bar.set_postfix({\"Accuracy\": eval_metric})\n        \n        return eval_metric\n\n    def predict_proba(self, inputs):\n        self.model.eval()\n        with torch.no_grad():\n            inputs = inputs.to(self.device)\n            outputs = self.model(inputs)\n            #예측값을 반환하므로 targets은 필요 없음.\n            #targets = targets.to(self.device)\n            pred_proba = F.softmax(outputs, dim=-1) #또는 dim=1\n\n        return pred_proba\n\n    def predict(self, inputs):\n        pred_proba = self.predict_proba(inputs)\n        pred_class = torch.argmax(pred_proba, dim=-1)\n\n        return pred_class","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trained_model = trainer.get_trained_model()\n\n# 학습데이터와 동일하게 정규화된 데이터를 입력해야 함. \n# test_dataset = CIFAR10(root='./data', train=False, download=True, transform=ToTensor())\n# test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)\n\npredictor = Predictor(model=trained_model, device=device)\neval_metric = predictor.evaluate(test_loader)\nprint(f'test dataset evaluation:{eval_metric:.4f}')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Dropout\n* Pytorch는 Dropout을 위해 nn.Dropout(p) Layer 제공\n* nn.Dropout(p)는 지정된 p 확률로 입력 tensor의 element값을 0로, 0으로 변경되지 않은 element들은 scale factor로 scaling 수행","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\ninput_tensor = torch.randn(4, 10)\nprint(f\"input Tensor:\\n{input_tensor}\")\n\nnum_zeros = torch.sum(input_tensor == 0).item()\nprint(f\"Number of zeros in input_tensor: {num_zeros}\")\n\n# p=0.5로 Dropout Layer정의\ndropout = nn.Dropout(p=0.5)\n\n# Dropout Layer 적용. \noutput_tensor = dropout(input_tensor)\n\n# Dropout 적용 후 tensor의 element value가 0인 건수 조사\nnum_zeros = torch.sum(output_tensor == 0).item()\n\n# output tensor의 element 전체 건수\ntotal_elements = output_tensor.numel()\n\n# output tensor의 전체 element 중 0인 건수 비율 조사. \npercentage_zeros = (num_zeros / total_elements) * 100\n\nprint(f\"Output Tensor:\\n{output_tensor}\")\nprint(f\"Number of zeros in output tensor: {num_zeros}\")\nprint(f\"Percentage of zeros: {percentage_zeros:.2f}%\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Classifier 부분을 Dropout과 Linear로 연결\n* Pytorch는 기존 모델(Module)의 서브 모듈만 따로 동적으로 연결하여 모델을 변경할 수 있게 함.\n* 기존 모델에서 classification block 부분만 Dropout을 적용 할 수 있도록 모델 구조 변경","metadata":{}},{"cell_type":"code","source":"NUM_CLASSES = 10\n\nsimple_cnnbn_base = SimpleCNNWithBN(num_classes=NUM_CLASSES)\nsimple_cnnbn_base.classifier_block","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"do_classifier_block = nn.Sequential(\n            nn.Flatten(),\n            nn.Dropout(p=0.5),\n            nn.Linear(in_features=128*4*4, out_features=300),\n            nn.ReLU(),\n            nn.Dropout(p=0.3),\n            nn.Linear(in_features=300, out_features=10),\n        )\nsimple_cnnbn_base.classifier_block = do_classifier_block\nprint(simple_cnnbn_base.classifier_block)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"summary(model=simple_cnnbn_base, input_size=(1, 3, 32, 32), \n        col_names=['input_size', 'output_size', 'num_params'], \n        row_settings=['var_names'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def create_do_classifier_block(first_features, second_features, first_dos, second_dos, num_classes=10):\n    return nn.Sequential(\n            nn.Flatten(),\n            nn.Dropout(p=first_dos),\n            nn.Linear(in_features=first_features, out_features=second_features),\n            nn.ReLU(),\n            nn.Dropout(p=second_dos),\n            nn.Linear(in_features=second_features, out_features=num_classes),\n        )\n\nsimple_cnnbn_base = SimpleCNNWithBN(num_classes=NUM_CLASSES)\ndo_classifier_block = create_do_classifier_block(first_features=128*4*4, second_features=300,\n                                                 first_dos=0.5, second_dos=0.3, num_classes=10)\nsimple_cnnbn_base.classifier_block = do_classifier_block\n\nsummary(model=simple_cnnbn_base, input_size=(1, 3, 32, 32), \n        col_names=['input_size', 'output_size', 'num_params'], \n        row_settings=['var_names'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch \nimport torch.nn as nn\nfrom torch.optim import SGD, Adam\n\nNUM_INPUT_CHANNELS = 3\nNUM_CLASSES = 10\n\nmodel = SimpleCNNWithBN(num_classes=NUM_CLASSES)\ndo_classifier_block = create_do_classifier_block(first_features=128*4*4, second_features=300,\n                                                 first_dos=0.5, second_dos=0.3, num_classes=10)\nmodel.classifier_block = do_classifier_block\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\noptimizer = Adam(model.parameters(), lr=0.001)\nloss_fn = nn.CrossEntropyLoss()\n\ntrainer = Trainer(model=model, loss_fn=loss_fn, optimizer=optimizer,\n       train_loader=tr_loader, val_loader=val_loader, device=device)\n# 학습 및 평가 \nhistory = trainer.fit(30)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trained_model = trainer.get_trained_model()\n\n# 학습데이터와 동일하게 정규화된 데이터를 입력해야 함. \n# test_dataset = CIFAR10(root='./data', train=False, download=True, transform=ToTensor())\n# test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)\n\npredictor = Predictor(model=trained_model, device=device)\neval_metric = predictor.evaluate(test_loader)\nprint(f'test dataset evaluation:{eval_metric:.4f}')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}