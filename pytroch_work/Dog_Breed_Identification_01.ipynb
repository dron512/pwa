{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Stanford Dog Breed 데이터 세트를 아래 URL에서 직접 Download 및 압축 해제\n* Kaggle의 Dataset으로 Object Storage 연결 시 이미지를 한장 씩 읽는 데 많은 시간이 소요되어 모델 학습에 시간이 더 걸림. \n* Local Disk에 바로 이미지를 다운로드/압축 해제 후 모델에서 이를 이용할 수 있도록 함. ","metadata":{}},{"cell_type":"code","source":"# stanford dog breed 데이터 세트 다운로드 \n!wget http://vision.stanford.edu/aditya86/ImageNetDogs/images.tar\n# 현재 디렉토리인 /kaggle/working에 바로 압축 해제 \n!ls; tar -xvf images.tar","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 현재 디렉토리의 내용 확인. \n!ls; pwd","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!cd /kaggle/working/Images;ls","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\n\nfor dirname, _, filenames in os.walk('/kaggle/working/Images'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 이미지 파일들의 디렉토리와 파일명을 기반으로 메타 정보인 이미지 절대경로, 레이블을 DataFrame으로 생성\n* /kaggle/working/Images 디렉토리 밑에 Dog breed 서브 디렉토리와 이미지 파일로 구성 되어 있음. \n* 레이블 값은 이미지 파일의 절대경로에서 이미지 파일 바로 위에 있는 서브 디렉토리를 가공하여 생성. ","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport os \n\nIMAGE_DIR = '/kaggle/working/Images' \n\ndef make_dogbreed_dataframe(image_dir=IMAGE_DIR):\n    paths = []\n    label_gubuns = []\n    for dirname, _, filenames in os.walk(image_dir):\n        for filename in filenames:\n            # 이미지 파일이 아닌 파일도 해당 디렉토리에 있음.\n            if '.jpg' in filename:\n                # 파일의 절대 경로를 file_path 변수에 할당. \n                file_path = dirname+'/'+ filename\n                paths.append(file_path)\n                # 이미지 파일의 절대 경로에서 레이블명 생성을 위한 1차 추출. '/'로 분할하여 파일 바로 위 서브디렉토리 이름 가져옴.  \n                start_pos = file_path.find('/', 20)\n                end_pos = file_path.rfind('/')\n                imsi_breed = file_path[start_pos+1:end_pos]\n                # 1차 추출된 데이터를 기반으로 '-' 이후 데이터가 레이블 값임. \n                breed = imsi_breed[imsi_breed.find('-')+1:]\n                #print(start_pos, end_pos, imsi_breed, breed)\n                label_gubuns.append(breed)\n\n    data_df = pd.DataFrame({'path':paths, 'label':label_gubuns})\n    \n    #label에 따른 숫자 target 값 매핑\n    sorted_label = np.sort(data_df['label'].unique())\n    label_mapping = {label: index for index, label in enumerate(sorted_label)}\n    data_df['target'] = data_df['label'].map(label_mapping)\n    data_df = data_df.sort_values(by=['target', 'path'], ascending=True)\n    \n    return data_df\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\npd.set_option('display.max_colwidth', 200)\n\ndata_df = make_dogbreed_dataframe()\nprint('data_df shape:', data_df.shape)\ndata_df.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sorted_label = np.sort(data_df['label'].unique())\n\nlabel_mapping = {label: index for index, label in enumerate(sorted_label)}\nprint(label_mapping)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Dog Breed의 개별 분포도 확인. ","metadata":{}},{"cell_type":"code","source":"print(data_df.shape)\n# breed 별 건수 확인\ndata_df[['label', 'target']].value_counts()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 막대 그래프 형태로 breed별 건수 확인 \nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline \n\nplt.figure(figsize=(26, 4))\n\nsns.countplot(data=data_df, x='label')\nplt.xticks(rotation=90)\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Dog Breed의 이미지 보기","metadata":{}},{"cell_type":"code","source":"import cv2\n\n# dog breed 별로 image를 보기 위한 utility 함수 생성.\ndef show_grid_images(image_path_list, ncols=8, title=None):\n    figure, axs = plt.subplots(figsize=(22, 4), nrows=1, ncols=ncols)\n    for i in range(ncols):\n        image = cv2.cvtColor(cv2.imread(image_path_list[i]), cv2.COLOR_BGR2RGB)\n        axs[i].imshow(image)\n        axs[i].set_title(title)  ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"breed_image_list_01 = data_df[data_df['label']=='Staffordshire_bullterrier']['path'].iloc[:6].tolist()\nbreed_image_list_02 = data_df[data_df['label']=='American_Staffordshire_terrier']['path'].iloc[:6].tolist()\n\nshow_grid_images(breed_image_list_01, ncols=6, title='Staffordshire_bullterrier')\nshow_grid_images(breed_image_list_02, ncols=6, title='American_Staffordshire_terrier')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data_df['label'].value_counts().index.tolist()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"breed_list = data_df['label'].value_counts().index.tolist()\n\nfor iter_cnt, breed in enumerate(breed_list):\n    breed_image_list = data_df[data_df['label']==breed]['path'].iloc[:6].tolist()\n    show_grid_images(breed_image_list, ncols=6, title=breed)\n    if iter_cnt == 4:\n        break","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Augmentation 적용한 이미지 살펴 보기","metadata":{}},{"cell_type":"code","source":"import albumentations as A\n\n# crop은 사용에 주의할것. 사람과 개가 같이 있으므로 center에 사람이 있을 경우 사람만 이미지가 잘릴수 있음. \nimsi_augmentor = A.Compose([\n    A.Resize(height=224, width=224, p=1), \n    A.CenterCrop(height=200, width=200, p=1),#A.CenterCrop(height=180, width=180, p=1)\n    A.HorizontalFlip(p=0.5),\n    A.VerticalFlip(p=0.5),\n    A.ShiftScaleRotate(p=0.5),\n    A.RandomBrightnessContrast(brightness_limit=(-0.2, 0.2), contrast_limit=(-0.2, 0.2), p=0.5),\n    A.HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit=0.2, val_shift_limit=0.2, p=0.5)\n])\n\n# augmented 적용된 이미지를 보기 위해서 위에서 albumetation으로 image 변환 기법 적용된 transformer 입력\n# 이미지 사이즈를 224x224로 resize 적용. \ndef show_grid_images(image_path_list, augmentor=None, ncols=4, title=None):\n    figure, axs = plt.subplots(figsize=(22, 4), nrows=1, ncols=ncols)\n    for i in range(ncols):\n        image = cv2.cvtColor(cv2.imread(image_path_list[i]), cv2.COLOR_BGR2RGB)\n        if augmentor is not None:\n            image = augmentor(image=image)['image']\n        axs[i].imshow(image)\n        axs[i].axis('off')\n        axs[i].set_title(title) \n        \nbreed_image_list_01 = data_df[data_df['label']=='Staffordshire_bullterrier']['path'].iloc[:6].tolist()       \nshow_grid_images(breed_image_list_01, augmentor=None, ncols=6, title='orignal Staffordshire_bullterrier')\nshow_grid_images(breed_image_list_01, augmentor=imsi_augmentor, ncols=6, title='augmented Staffordshire_bullterrier')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 전체 DataFrame을 학습과 테스트용 DataFrame으로 분리. 학습 DataFrame은 다시 학습과 검증용으로 분리 \n* train_test_split()을 이용하여 전체의 40%를 테스트 데이터로 할당. stratify인자로 breed label별로 균등하게 할당 설정. ","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# 전체 데이터의 60%를 학습, 40%를 테스트로 분리. \ntrain_df, test_df = train_test_split(data_df, test_size=0.4, stratify=data_df['label'], random_state=2025)\nprint(train_df.shape, test_df.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(train_df['label'].value_counts()/train_df.shape[0])\nprint(test_df['label'].value_counts()/test_df.shape[0])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 다시 학습 데이터의 80%를 학습, 20%를 검증으로 분리\ntr_df, val_df = train_test_split(train_df, test_size=0.2, stratify=train_df['target'], random_state=2025)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Custom Dataset와 DataLoader 생성\n* augmentation은 light한 augmention 부터 시작하여 점차 강도를 높임","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\nimport cv2\n\nclass BreedDataset(Dataset):\n    # 이미지 파일리스트, 타겟 파일리스트, transforms 등 이미지와 타겟 데이터 가공에 필요한 인자들을 입력 받음\n    def __init__(self, image_paths, targets=None, transform=None):\n        self.image_paths = image_paths\n        self.targets = targets\n        self.transform = transform\n    \n    # 전체 건수를 반환\n    def __len__(self):\n        return len(self.image_paths)\n        \n    # idx로 지정된 하나의 image, label을 tensor 형태로 반환\n    def __getitem__(self, idx):\n        image_path = self.image_paths[idx]\n        # opencv로 이미지 파일 로딩\n        image_np = cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2RGB).astype(np.float32)\n        # 보통은 transform이 None이 되는 경우는 거의 없음(Tensor 변환이라도 있음)\n        image = self.transform(image=image_np)['image']\n\n        if self.targets is not None:\n            # 개별 target값을 tensor로 변환.\n            target = torch.tensor(self.targets[idx])\n            return image, target\n        # 테스트 데이터의 경우 targets가 입력 되지 않을 수 있으므로 이를 대비. \n        else:\n            return image","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\nBATCH_SIZE = 16\nclass CFG:\n    batch_size = 16\n    image_size = 224\n\n# Data Augmentation\ntr_transform = A.Compose([\n    A.Resize(CFG.image_size, CFG.image_size, p=1),\n    A.HorizontalFlip(p=0.5),\n    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n    ToTensorV2()\n])\n\nval_transform = A.Compose([\n    A.Resize(CFG.image_size, CFG.image_size),\n    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n    ToTensorV2()\n])\n\ndef create_tr_val_loader(tr_df, val_df, tr_transform, val_transform):\n    tr_dataset =BreedDataset(image_paths=tr_df['path'].to_list(), \n                               targets=tr_df['target'].to_list(), transform=tr_transform)\n    val_dataset = BreedDataset(image_paths=val_df['path'].to_list(), \n                               targets=val_df['target'].to_list(), transform=val_transform)\n    \n    tr_loader = DataLoader(tr_dataset, batch_size = BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)\n    val_loader = DataLoader(val_dataset, batch_size=4*BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)\n\n    return tr_loader, val_loader\n\ntr_loader, val_loader = create_tr_val_loader(tr_df=tr_df, val_df=val_df, \n                                             tr_transform=tr_transform, val_transform=val_transform)\nimages, labels = next(iter(tr_loader))\nprint(images.shape, labels.shape)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### torchvision Model 생성.\n* Resnet101과 Efficientnet 계열을 테스트 할 수 있도록 모델 생성 함수 생성.  ","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torchvision import models\n\ndef create_tv_model(model_name, num_classes=1000):\n    model = None\n    if model_name == 'efficientnet_v2_s':\n        model = models.efficientnet_v2_s(weights='DEFAULT')\n        model.classifier = nn.Sequential(nn.Dropout(p=0.2),\n                                         nn.Linear(in_features=1280, out_features=num_classes))\n    elif model_name == 'efficientnet_b4':\n        model = models.efficientnet_b4(weights='DEFAULT')\n        model.classifier = nn.Sequential(nn.Dropout(p=0.2),\n                                         nn.Linear(in_features=1792, out_features=num_classes))\n    elif model_name == 'efficientnet_b1':\n        model = models.efficientnet_b1(weights='DEFAULT')\n        model.classifier = nn.Sequential(nn.Dropout(p=0.2),\n                                         nn.Linear(in_features=1280, out_features=num_classes))\n    elif model_name == 'efficientnet_b0':\n        model = models.efficientnet_b0(weights='DEFAULT')\n        model.classifier = nn.Sequential(nn.Dropout(p=0.2),\n                                         nn.Linear(in_features=1280, out_features=num_classes))\n    elif model_name == 'resnet101':\n        model = models.resnet101(weights='DEFAULT')\n        model.fc = nn.Linear(in_features=2048, out_features=num_classes)\n        \n    return model\n\neff_model = create_tv_model('efficientnet_b0', num_classes=120)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torchvision import models\n\nmodels.resnet101(weights=None)#resnet101, efficientnet_b0, efficientnet_v2_s","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"models.ResNet101_Weights.IMAGENET1K_V2.transforms() # EffcientNet_B0","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Trainer로 모델 학습. \n* resnet101과 efficientnet B0/B1","metadata":{}},{"cell_type":"code","source":"# /kaggle/working/modular/v1 디렉토리에 utils.py 파일 다운로드\n!rm -rf ./modular/v1\n!mkdir -p ./modular/v1\n!wget -O ./modular/v1/utils.py https://raw.githubusercontent.com/chulminkw/CNN_PG_Torch/main/modular/v1/utils.py?raw=true\n!ls ./modular/v1\n\nimport sys\n\n# 반드시 system path를 아래와 같이 잡아줘야 함. \nsys.path.append('/kaggle/working')\n\n#아래가 수행되는지 반드시 확인\nfrom modular.v1.utils import Trainer, ModelCheckpoint, EarlyStopping","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.optim import Adam\nimport torch.optim as optim\nimport torchmetrics\nimport timm\n\nCFG.batch_size = 32 # 16\nCFG.image_size = 224\n\n# Horizontal_flip\ntr_transform_01 = A.Compose([\n    A.Resize(CFG.image_size, CFG.image_size, p=1),\n    A.HorizontalFlip(p=0.5),\n    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n    ToTensorV2()\n])\n\nval_transform = A.Compose([\n    A.Resize(CFG.image_size, CFG.image_size),\n    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n    ToTensorV2()\n])\n\n#learning_rate, callbacks, epochs는 인자로 입력\ndef train_breed(model_name, tr_transform, val_transform, learning_rate=1e-3, callbacks=None, epochs=30):\n    tr_loader, val_loader = create_tr_val_loader(tr_df=tr_df, val_df=val_df, \n                                             tr_transform=tr_transform, val_transform=val_transform)\n    model = create_tv_model(model_name, num_classes=120)\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    optimizer = Adam(model.parameters(), lr=learning_rate)\n    loss_fn = nn.CrossEntropyLoss()\n    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n                optimizer=optimizer, mode='min', factor=0.2, patience=3, threshold=0.01, min_lr=1e-7)\n    \n    trainer = Trainer(model=model, loss_fn=loss_fn, optimizer=optimizer,\n                   train_loader=tr_loader, val_loader=val_loader, scheduler=scheduler, \n                   callbacks=callbacks, device=device)\n    # 학습 및 평가. epochs는 20회로 조정. \n    history = trainer.fit(epochs)\n    \n    return trainer, history","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#모델은 resnet101, HorizontalFlip만 augmentation적용. learning_rate는 1e-4, callbacks는 None, epochs는 30회 수행. \ntrainer, history = train_breed('resnet101', tr_transform_01, val_transform, learning_rate=1e-4, \n                               callbacks=None, epochs=30)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#아래가 수행되는지 반드시 확인\nfrom modular.v1.utils import Predictor\n\ntest_image_paths = test_df['path'].to_list()\ntest_targets = test_df['target'].to_list()\n\n#CFG.batch_size = 16\nCFG.image_size = 224\n\ntest_transform = A.Compose([\n    A.Resize(CFG.image_size, CFG.image_size),\n    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n    ToTensorV2()\n])\n\ntest_dataset = BreedDataset(image_paths=test_image_paths, \n                            targets=test_targets, transform=test_transform)\ntest_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=4, pin_memory=True)\n\ntrained_model = trainer.get_trained_model()\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\npredictor = Predictor(model=trained_model, device=device)\neval_metric = predictor.evaluate(test_loader)\nprint(f'test dataset evaluation:{eval_metric:.4f}')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### 어떤 Breed가 예측이 많이 틀렸는지 확인 \n* 실제 Ground Truth Breed와 예측 Breed가 어떻게 틀려졌는지 확인. ","metadata":{}},{"cell_type":"code","source":"# 모든 테스트 데이터에 대해서 예측 수행 후 numpy로 반환 \ndef get_all_predictions(predictor, test_loader):\n    preds_all_list = []\n    targets_all_list = []\n    \n    for images, targets in test_loader:\n        preds = predictor.predict(images).cpu().numpy()\n        # 개별 원소값으로 저장해야 하므로 append()가 아니라 extend()로 list에 저장. \n        preds_all_list.extend(preds)\n        # targets_all_list.extend(targets.cpu().numpy())\n\n    preds_all = np.array(preds_all_list)\n\n    return preds_all\n\npreds_all = get_all_predictions(predictor, test_loader)\n\n#예측 결과를 test_df의 별도 컬럼으로 저장. \ntest_df['resnet101_pred'] = preds_all","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_df.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 예측이 틀린 데이터 확인 \ntest_df[test_df['target'] != test_df['resnet101_pred']]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_df[test_df['target'] != test_df['resnet101_pred']]['label'].value_counts()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_df[test_df['label'] == 'Eskimo_dog'][['target', 'resnet101_pred']].value_counts()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sorted_label = np.sort(data_df['label'].unique())\n#print(sorted_label)\nprint(sorted_label[[24, 64, 101]])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def show_grid_images(image_path_list, augmentor=None, ncols=4, title=None):\n    figure, axs = plt.subplots(figsize=(22, 4), nrows=1, ncols=ncols)\n    for i in range(ncols):\n        image = cv2.cvtColor(cv2.imread(image_path_list[i]), cv2.COLOR_BGR2RGB)\n        image = cv2.resize(image, (224, 224))\n        if augmentor is not None:\n            image = augmentor(image=image)['image']\n        axs[i].imshow(image)\n        axs[i].axis('off')\n        axs[i].set_title(title) \n        \nbreed_image_list_01 = data_df[data_df['label']=='Siberian_husky']['path'].iloc[:6].tolist()\nbreed_image_list_02 = data_df[data_df['label']=='Eskimo_dog']['path'].iloc[:6].tolist()\n\nshow_grid_images(breed_image_list_01, ncols=6, title='Siberian_husky')\nshow_grid_images(breed_image_list_02, ncols=6, title='Eskimo_dog')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline \n\nwrong_result_df = test_df[test_df['target'] != test_df['resnet101_pred']]\n\nplt.figure(figsize=(26, 4))\nplt.xticks(rotation=90)\nsns.countplot(data=wrong_result_df, x='label')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### efficient b0로 모델 학습/평가\n* 동일한 batch size, image size, augmentation으로 efficientnet_b0 모델 학습 및 평가 ","metadata":{}},{"cell_type":"code","source":"models.EfficientNet_B0_Weights.IMAGENET1K_V1.transforms() # EffcientNet_B0","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.optim import Adam\nimport torch.optim as optim\nimport torchmetrics\nimport timm\n\n# 동일한 batch size, image size, augmentation으로 efficientnet_b0 모델 학습 및 평가. \ntrainer, history = train_breed('efficientnet_b0', tr_transform_01, val_transform, learning_rate=1e-4, \n                               callbacks=None, epochs=30)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trained_model = trainer.get_trained_model()\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\npredictor = Predictor(model=trained_model, device=device)\neval_metric = predictor.evaluate(test_loader)\nprint(f'test dataset evaluation:{eval_metric:.4f}')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### EfficientNet B1 으로 학습/평가\n* image size를 240으로 설정","metadata":{}},{"cell_type":"code","source":"models.EfficientNet_B1_Weights.IMAGENET1K_V2.transforms() # EffcientNet_B0","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.optim import Adam\nimport torch.optim as optim\nimport torchmetrics\nimport timm\n\nCFG.batch_size = 32 # 16\nCFG.image_size = 240\n\n# Horizontal_flip\ntr_transform_01 = A.Compose([\n    A.Resize(CFG.image_size, CFG.image_size, p=1),\n    A.HorizontalFlip(p=0.5),\n    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n    ToTensorV2()\n])\n\nval_transform = A.Compose([\n    A.Resize(CFG.image_size, CFG.image_size),\n    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n    ToTensorV2()\n])\n\ntrainer, history = train_breed('efficientnet_b1', tr_transform_01, val_transform, learning_rate=1e-4, \n                               callbacks=None, epochs=30)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#아래가 수행되는지 반드시 확인\nfrom modular.v1.utils import Predictor\n\n# 테스트 이미지 사이즈를 240으로 증가. \nCFG.image_size = 240\n\ntest_transform = A.Compose([\n    A.Resize(CFG.image_size, CFG.image_size),\n    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n    ToTensorV2()\n])\n\ntest_dataset = BreedDataset(image_paths=test_image_paths, \n                            targets=test_targets, transform=test_transform)\ntest_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=4, pin_memory=True)\n\ntrained_model = trainer.get_trained_model()\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\npredictor = Predictor(model=trained_model, device=device)\neval_metric = predictor.evaluate(test_loader)\nprint(f'test dataset evaluation:{eval_metric:.4f}')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 다양한 Augmentation을 적용하여 모델 학습 및 성능 평가\n* Heavy한 Augmentation을 적용한다고 반드시 성능이 향상되는 것이 아님(오히려 성능이 저하될 수 있음)","metadata":{}},{"cell_type":"code","source":"from torch.optim import Adam\nimport torch.optim as optim\n\nCFG.batch_size = 32 # 16\nCFG.image_size = 240\n\n# Horizontal_flip\nfrom torch.optim import Adam\nimport torch.optim as optim\nimport torchmetrics\nimport timm\n\nCFG.batch_size = 32 # 16\nCFG.image_size = 240\n\ntr_transform_02 = A.Compose([\n    A.RandomResizedCrop(height=180, width=180, scale=(0.5, 1.0), p=0.3),\n    A.HorizontalFlip(p=0.3),\n    A.VerticalFlip(p=0.3),\n    A.ShiftScaleRotate(p=0.2),\n    A.RandomBrightnessContrast(brightness_limit=(-0.2, 0.2), contrast_limit=(-0.2, 0.2), p=0.2),\n    A.ColorJitter(p=0.2),\n    A.OneOf(\n        [A.CoarseDropout(p=1, max_holes=26), \n         A.CLAHE(p=1)\n        ], p=0.3), \n    A.Resize(CFG.image_size, CFG.image_size),\n    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n    ToTensorV2()\n])\n\nval_transform = A.Compose([\n    A.Resize(CFG.image_size, CFG.image_size),\n    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n    ToTensorV2()\n])","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"breed_image_list_01 = data_df[data_df['label']=='Staffordshire_bullterrier']['path'].iloc[:6].tolist()       \nshow_grid_images(breed_image_list_01, augmentor=None, ncols=6, title='orignal Staffordshire_bullterrier')\nshow_grid_images(breed_image_list_01, augmentor=tr_transform_02, ncols=6, title='augmented')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer, history = train_breed('efficientnet_b1', tr_transform_02, val_transform, learning_rate=1e-4, epochs=30)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#아래가 수행되는지 반드시 확인\nfrom modular.v1.utils import Predictor\n\ntest_image_paths = test_df['path'].to_list()\ntest_targets = test_df['target'].to_list()\n\n# 테스트 이미지 사이즈를 240으로 증가. \nCFG.image_size = 240\n\ntest_transform = A.Compose([\n    A.Resize(CFG.image_size, CFG.image_size),\n    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n    ToTensorV2()\n])\n\ntest_dataset = BreedDataset(image_paths=test_image_paths, \n                            targets=test_targets, transform=test_transform)\ntest_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=4, pin_memory=True)\n\ntrained_model = trainer.get_trained_model()\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\npredictor = Predictor(model=trained_model, device=device)\neval_metric = predictor.evaluate(test_loader)\nprint(f'test dataset evaluation:{eval_metric:.4f}')","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### 다른 Augumentation 적용 후 모델 학습 평가","metadata":{}},{"cell_type":"code","source":"tr_transform_03 = A.Compose([\n    A.HorizontalFlip(p=0.3),\n    A.ShiftScaleRotate(scale_limit=(0.7, 0.9), p=0.2, rotate_limit=30),\n    A.RandomBrightnessContrast(brightness_limit=(-0.2, 0.2), contrast_limit=(-0.2, 0.2), p=0.2),\n    A.ColorJitter(p=0.2),\n    A.Resize(CFG.image_size, CFG.image_size, p=1),\n    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n    ToTensorV2()\n])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"breed_image_list_01 = data_df[data_df['label']=='Staffordshire_bullterrier']['path'].iloc[:6].tolist()       \nshow_grid_images(breed_image_list_01, augmentor=None, ncols=6, title='orignal Staffordshire_bullterrier')\nshow_grid_images(breed_image_list_01, augmentor=tr_transform_03, ncols=6, title='augmented')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer, history = train_breed('efficientnet_b1', tr_transform_03, val_transform, learning_rate=1e-4, epochs=30)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trained_model = trainer.get_trained_model()\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\npredictor = Predictor(model=trained_model, device=device)\neval_metric = predictor.evaluate(test_loader)\nprint(f'test dataset evaluation:{eval_metric:.4f}')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}