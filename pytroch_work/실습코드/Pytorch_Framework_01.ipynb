{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### nn.Module을 상속받아 Custom Model 생성하기\n* 입력 Feature 갯수가 784이고 출력 feature 갯수가 100인 Linear Layer와 ReLU Activation Layer, 최종 10개의 출력 feature를 가지는 Linear Layer를 기반으로 모델 생성.\n* \\_\\_init\\_\\_(self,..)에서 해당 Layer들 선언\n* forward(self, x)에서 입력 tensor의 forward pass를 기술하면서 이들 Layer들을 연결\n* 모델 입력은 반드시 tensor가 되어야 하며, 출력도 tensor가 됨. ","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n#from torch import nn\n\n# Custom Model 생성. \nclass LinearModel(nn.Module):\n    def __init__(self, num_classes=10):\n        # 반드시 super()를 호출. \n        super().__init__()\n        #Linear Layer와 ReLU Layer 생성. \n        self.linear_01 = nn.Linear(in_features=784, out_features=100)\n        self.relu_01 = nn.ReLU()\n        self.linear_02 = nn.Linear(in_features=100, out_features=num_classes)\n        \n    # 순방향 전파(Pass Forward) 기술.\n    def forward(self, x):\n        x = self.linear_01(x)\n        x = self.relu_01(x)\n        output = self.linear_02(x)\n        return output","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#임의의 입력 tensor 생성. \ninput_tensor = torch.randn(size=(64, 784))\nprint(input_tensor.size())\n\n# LinearModel 객체 생성. __init__(self, num_classes)에 선언된 객체 초기화 인자 입력하여 생성. \nlinear_model = LinearModel(num_classes=10)\n\n# LinearModel 객체는 Callable Object이므로 LinearModel 객체에 함수 호출과 유사한 형태로 입력 인자 전달하여 forward()메소드 호출. \noutput_tensor = linear_model(input_tensor)\nprint(output_tensor.size())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Layer(nn.Linear, nn.Conv2d, nn.ReLU, nn.MaxPool2d등) 살펴보기\n* Pytorch의 Layer는 신경망(Neural Network)을 쉽게 생성하기 위한 직관적인 building block\n* Layer 역시 nn.Module을 상속 받아 생성되며, 자동미분(Auto differentiation)과 GPU device 지원\n* Layer들은 입력 데이터에 적용되는 구조와 변환을 기술하여 선형 변환(Liner transformation), Convolution 적용, 활성함수 적용(Activation), Pooling과 Normalization 등의 작업을 수행.\n* 내부적으로 학습 파라미터를 가지고 있는 Layer(nn.Linear, nn.Conv2d)와 주로 변환만을 수행하는 Layer(nn.ReLU, nn.MaxPool2d)등이 있음.\n* Callable Object의 생성과 입력 데이터 전달 방식과 비슷하게, 생성 인자를 입력하여 Layer객체를 생성 한 뒤 변환될 입력 tensor를 객체의 인자로 입력해 주는 방식으로 변환","metadata":{}},{"cell_type":"code","source":"import torch.nn as nn\n\nlinear_01= nn.Linear(in_features=784, out_features=100)\n# 학습 파라미터를 가지고 있음.\nprint(linear_01.weight)\nprint(linear_01.bias)\n\n# 학습 파라미터는 nn.parameter.Parameter 타입이며, nn.parameter.Parameter는 학습이 가능한(자동 미분) 특별한 타입의 Tensor임. \nprint(type(linear_01.weight))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# linear_01= nn.Linear(in_features=784, out_features=100)\n# weight의 shape는 matmul()을 위해서 (in_features, out_features)의 행렬 위치가 바뀌는 Transpose 적용되어야 함. \nprint(linear_01.weight.shape, linear_01.bias.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### nn.Parameter 는 학습이 가능한(자동 미분) 특별한 타입의 Tensor임. \n* nn.Module을 상속받은 모든 객체는 자신이 가지는 Parameter Tensor를 Optimizer에 등록할 수 있음.\n* 일반 Tensor역시 requires_grad를 수행하면 자동 미분은 가능하지만 Optimizer에 등록 할 수는 없으므로 optimizer에서 grad upgrade를 수행할 수 없음.","metadata":{}},{"cell_type":"code","source":"# Layer의 parameters() 메소드는 Layer가 가지는 모든 parameter들을 iteration으로 반환함. \nfor parameter in linear_01.parameters():\n    print(parameter)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\n\ntensor_01 = torch.rand(size=(100, 784))\nprint(tensor_01.requires_grad)\n\nparam_01 = nn.Parameter(data=tensor_01)\nprint(param_01.shape, param_01.requires_grad)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 모델 구성 모듈(Layer 및 서브모듈) 살펴보기\n* 서브모듈(submodule)은 nn.Module을 상속 받아 생성된 컴포넌트(Layer도 서브모듈). 보통은 여러개의 Layer등을 엮어서 만들어지는 또 다른 클래스(블록)라는 의미로 통용되며, 모델이 복잡한 구조로 되어 있을 경우 특정 구조를 작게 블록화 시키는데 적용됨. \n* 서브모듈 또한 nn.Module에서 제공하는 여러기능을 가지게 됨(자식 모듈 등록, 파라미터 자동 등록 및 자동 미분 수행등)\n* 서브모듈 생성 시 반드시 생성자 메소드와  forward() 메소드를 구현해야 함.\n* 강의 진행 시 용어 정리는 아래와 같이 하겠음.\n    * 모델: 최종으로 만들어지는 네트웍 모델\n    * 서브모듈(또는 Block): 여러개의 Layer로 연결되어 만들어 지는 블록형 모듈.\n    * 서브모듈(모듈): Layer와 서브모듈등 nn.Module을 상속받은 모든 객체\n    * Layer: nn.Linear와 같은 Layer\n    * nn.Module: nn.Module","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\n# 서브 모듈 생성.\nclass SimpleBlock(nn.Module):\n    def __init__(self, in_features, out_features):\n        super().__init__()\n        self.linear_01 = nn.Linear(in_features=in_features,\n                                   out_features=out_features)\n        self.relu_01 = nn.ReLU()\n\n    def forward(self, x):\n        x = self.linear_01(x)\n        x = self.relu_01(x)\n        return x\n\nclass LinearModel(nn.Module):\n    def __init__(self, num_classes=10):\n        # 반드시 super()를 호출. \n        super().__init__()\n        # 서브 모듈인 SimpleBlock 생성\n        self.simple_01 = SimpleBlock(in_features=784,\n                                     out_features=100)\n        self.linear_02 = nn.Linear(in_features=100, out_features=num_classes)\n        \n    # 순방향 전파(Pass Forward) 기술.\n    def forward(self, x):\n        x = self.simple_01(x)\n        output = self.linear_02(x)\n        return output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T08:19:31.339569Z","iopub.execute_input":"2025-02-17T08:19:31.339863Z","iopub.status.idle":"2025-02-17T08:19:34.327504Z","shell.execute_reply.started":"2025-02-17T08:19:31.339834Z","shell.execute_reply":"2025-02-17T08:19:34.326652Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"input_tensor = torch.randn(size=(64, 784))\nprint(input_tensor.size())\n\n# LinearModel 객체 생성. __init__(self, num_classes)에 선언된 객체 초기화 인자 입력하여 생성. \nlinear_model = LinearModel(num_classes=10)\n\n# LinearModel 객체는 Callable Object이므로 LinearModel 객체에 함수 호출과 유사한 형태로 입력 인자 전달하여 forward()메소드 호출. \noutput_tensor = linear_model(input_tensor)\nprint(output_tensor.size())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T08:19:34.331063Z","iopub.execute_input":"2025-02-17T08:19:34.331288Z","iopub.status.idle":"2025-02-17T08:19:34.412748Z","shell.execute_reply.started":"2025-02-17T08:19:34.331267Z","shell.execute_reply":"2025-02-17T08:19:34.411741Z"}},"outputs":[{"name":"stdout","text":"torch.Size([64, 784])\ntorch.Size([64, 10])\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"#### 모델이 가지는 모든 모듈(Layer, 서브모듈) 확인하기\n* modules() 메소드는 자신의 모듈을 포함하여 Nesting된 서브 모듈(Layer포함하여 nn.Module을 상속받은 모든 클래스)를 출력\n* named_modules() 메소드는 자신의 모듈을 포함하여 Nesting된 서브 모듈까지 모듈명과 모듈 클래스를 출력\n* 모델이 가지는 내부 멤버변수(self로 지정된 변수)는 객체명.변수명으로 바로 접근 가능","metadata":{}},{"cell_type":"code","source":"# 모델 구성 출력\nprint(linear_model)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 자신의 모듈을 포함하여 Nesting된 서브 모듈까지 모두 출력\nfor module in linear_model.modules():\n    print(module)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# named_modules() 메소드는 자신의 모듈을 포함하여 Nesting된 서브 모듈까지 모듈명과 모듈 클래스를 출력\nfor name, module in linear_model.named_modules():\n    print(f\"Module Name: {name}, Module: {module}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# named_children() 메소드는 자기 직계 서브 모듈만 모듈명과 모듈 클래스 출력\nfor name, module in linear_model.named_children():\n    print(f\"Submodule Name: {name}, Submodule: {module}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#모델이 가지는 내부 멤버변수(self로 지정된 변수)는 객체명.변수명으로 바로 접근 가능\nprint('simple_01:', linear_model.simple_01)\nprint('linear_02:', linear_model.linear_02)\nprint('linear_01 in simple_01:', linear_model.simple_01.linear_01)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### 모델의 모든 Parameter 가져오기\n* nn.Module을 상속 받은 모든 클래스는 등록된 Parameter Tensor를 parameters()로 가져 올 수 있음.\n* 모델이 가지는 서브모듈들의 모든 parameter들을 parameters()로 가져 올 수 있음.\n* named_parameter()는 parameter를 가지는 서브모듈/Layer의 weight/bias와 parameter tensor를 모두 출력 ","metadata":{}},{"cell_type":"code","source":"for parameter in linear_model.parameters():\n    print(parameter)\n# for name, parameter in linear_model.named_parameters():\n#     print(name, parameter)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### torchinfo의 summary()\n* torchinfo 패키지의 summary()를 이용하여 보다 자세히 모델 구조를 확인 할 수 있음.\n    * model: 모델 객체\n    * input_size: 입력 tensor 사이즈. 일반적으로 batch 를 감안하여 입력. \n    * col_names: summary 수행 출력 컬럼명들. list형태로 입력.\n        * input_size: 입력 tensor size\n        * output_size: 출력 tensor size\n        * num_params: 학습 파라미터 갯수\n        * trainable: 학습 파라미터의 train 가능 설정(requires_grad) \n    * row_settings: row에 보여질 내용\n        * var_names: 모듈 변수명\n        * depth: 서브 모듈내 depth","metadata":{}},{"cell_type":"code","source":"#!pip install torchinfo","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"input_tensor = torch.randn(size=(64, 784))\nprint(input_tensor.size())\n\n# LinearModel 객체 생성. __init__(self, num_classes)에 선언된 객체 초기화 인자 입력하여 생성. \nlinear_model = LinearModel(num_classes=10)\n\n# LinearModel 객체는 Callable Object이므로 LinearModel 객체에 함수 호출과 유사한 형태로 입력 인자 전달하여 forward()메소드 호출. \noutput_tensor = linear_model(input_tensor)\nprint(output_tensor.size())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T09:03:33.591990Z","iopub.execute_input":"2025-02-17T09:03:33.592265Z","iopub.status.idle":"2025-02-17T09:03:33.599866Z","shell.execute_reply.started":"2025-02-17T09:03:33.592244Z","shell.execute_reply":"2025-02-17T09:03:33.599011Z"}},"outputs":[{"name":"stdout","text":"torch.Size([64, 784])\ntorch.Size([64, 10])\n","output_type":"stream"}],"execution_count":52},{"cell_type":"code","source":"from torchinfo import summary\n\nsummary(model=linear_model, input_size=(64, 784),\n        col_names=['input_size', 'output_size', 'num_params'], #'trainable'\n        row_settings=['var_names', 'depth'],\n        depth=3\n       )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T09:15:00.246396Z","iopub.execute_input":"2025-02-17T09:15:00.246723Z","iopub.status.idle":"2025-02-17T09:15:00.253861Z","shell.execute_reply.started":"2025-02-17T09:15:00.246693Z","shell.execute_reply":"2025-02-17T09:15:00.253161Z"}},"outputs":[{"execution_count":64,"output_type":"execute_result","data":{"text/plain":"===================================================================================================================\nLayer (type (var_name):depth-idx)        Input Shape               Output Shape              Param #\n===================================================================================================================\nLinearModel (LinearModel)                [64, 784]                 [64, 10]                  --\n├─SimpleBlock (simple_01): 1-1           [64, 784]                 [64, 100]                 --\n│    └─Linear (linear_01): 2-1           [64, 784]                 [64, 100]                 78,500\n│    └─ReLU (relu_01): 2-2               [64, 100]                 [64, 100]                 --\n├─Linear (linear_02): 1-2                [64, 100]                 [64, 10]                  1,010\n===================================================================================================================\nTotal params: 79,510\nTrainable params: 79,510\nNon-trainable params: 0\nTotal mult-adds (M): 5.09\n===================================================================================================================\nInput size (MB): 0.20\nForward/backward pass size (MB): 0.06\nParams size (MB): 0.32\nEstimated Total Size (MB): 0.58\n==================================================================================================================="},"metadata":{}}],"execution_count":64},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}