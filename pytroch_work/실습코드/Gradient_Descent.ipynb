{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### 보스턴 주택 가격 데이터 세트를 Peceptron 기반에서 학습 및 테스트하기 위한 데이터 로드\n* 보스턴 주택 가격 데이터 csv파일을 다운로드하고 이를 DataFrame으로 생성","metadata":{}},{"cell_type":"code","source":"!wget -O boston_price.csv https://raw.githubusercontent.com/chulminkw/CNN_PG_Torch/refs/heads/main/data/boston_house_price.csv","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nboston_df = pd.read_csv('./boston_price.csv')\nboston_df.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(boston_df.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Weight와 Bias의 Update 값을 계산하는 함수 생성.\n* w1은 RM(방의 계수) 피처의 Weight 값\n* w2는 LSTAT(하위계층 비율) 피처의 Weight 값\n* bias는 Bias\n* N은 입력 데이터 건수\n![](https://raw.githubusercontent.com/chulminkw/CNN_PG_Torch/main/image/gradient_descent.png)\n","metadata":{}},{"cell_type":"code","source":"import torch\n\n# gradient_descent()함수에서 반복적으로 호출되면서 update될 weight/bias 값을 계산하는 함수. \n# rm은 RM(방 개수), lstat(하위계층 비율), target은 PRICE임. 전체 해당 tensor가 다 입력됨. \n# 반환 값은 weight와 bias가 update되어야 할 값과 Mean Squared Error 값을 loss로 반환.\ndef get_update_weights_value(bias, w1, w2, rm, lstat, target, learning_rate=0.01):\n    # 데이터 건수\n    N = target.shape[0]\n    # 예측 값. \n    predicted = w1 * rm + w2 * lstat + bias\n    # 실제값과 예측값의 차이\n    diff = target - predicted \n    \n    # weight와 bias를 얼마나 update할 것인지를 계산.  \n    w1_update = -(2/N) * learning_rate * (torch.matmul(rm, diff))\n    w2_update = -(2/N) * learning_rate * (torch.matmul(lstat, diff))\n    bias_update = -(2/N) * learning_rate * torch.sum(diff)\n    \n    # Mean Squared Error값을 계산. \n    mse_loss = torch.mean(diff ** 2)\n    \n    # weight와 bias가 update되어야 할 값과 Mean Squared Error 값을 반환. \n    return bias_update, w1_update, w2_update, mse_loss","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Gradient Descent 를 적용하는 함수 생성\n* iter_epochs 수만큼 반복적으로 get_update_weights_value()를 호출하여 update될 weight/bias값을 구한 뒤 Weight/Bias를 Update적용. ","metadata":{}},{"cell_type":"code","source":"# RM, LSTAT feature tensor와 PRICE target tensor를 입력 받아서 iter_epochs수만큼 반복적으로 Weight와 Bias를 update적용. \ndef gradient_descent(features, target, iter_epochs=1000, learning_rate=0.01, verbose=True):\n    # w1, w2는 1차원 tensor로 변환하되 초기 값은 0으로 설정\n    # bias도 1차원 tensor로 변환하되 초기 값은 1로 설정. \n    w1 = torch.zeros(1, dtype=torch.float32)\n    w2 = torch.zeros(1, dtype=torch.float32)\n    bias = torch.ones(1, dtype=torch.float32)\n    print('최초 w1, w2, bias:', w1.item(), w2.item(), bias.item())\n    \n    # learning_rate와 RM, LSTAT 피처 지정. 호출 시 tensor형태로 RM과 LSTAT으로 된 2차원 feature가 입력됨.\n    rm = features[:, 0]\n    lstat = features[:, 1]\n    \n    # iter_epochs 수만큼 반복하면서 weight와 bias update 수행. \n    for i in range(1, iter_epochs+1):\n        # weight/bias update 값 계산\n        bias_update, w1_update, w2_update, loss = get_update_weights_value(bias, w1, w2, \n                                                                           rm, lstat, target, learning_rate=0.01)\n        \n        # weight/bias의 update 적용.\n        w1 = w1 - w1_update\n        w2 = w2 - w2_update\n        bias = bias - bias_update\n        if verbose: # 10회 epochs 시마다 출력\n            if i % 10 == 0:\n                print(f'Epoch: {i}/{iter_epochs}')\n                print(f'w1: {w1.item()}, w2: {w2.item()}, bias: {bias.item()}, loss: {loss.item()}')\n        \n    return w1, w2, bias","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Gradient Descent 적용\n* 신경망은 데이터를 정규화/표준화 작업을 미리 선행해 주어야 함. \n* 이를 위해 사이킷런의 MinMaxScaler를 이용하여 개별 feature값은 0~1사이 값으로 변환후 학습 적용.","metadata":{}},{"cell_type":"code","source":"import torch\nfrom sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler()\nscaled_features_np = scaler.fit_transform(boston_df[['RM', 'LSTAT']])\n\nscaled_features_ts = torch.from_numpy(scaled_features_np)\ntargets_ts = torch.from_numpy(boston_df['PRICE'].values)\n\nw1, w2, bias = gradient_descent(scaled_features_ts, targets_ts, iter_epochs=5000, verbose=True)\nprint('##### 최종 w1, w2, bias #######')\nprint(w1.item(), w2.item(), bias.item())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 계산된 Weight와 Bias를 이용하여 Price 예측\n* 예측 feature 역시 0~1사이의 scaled값을 이용하고 Weight와 bias를 적용하여 예측값 계산. ","metadata":{}},{"cell_type":"code","source":"predicted = scaled_features_ts[:, 0]*w1 + scaled_features_ts[:, 1]*w2 + bias\nboston_df['PREDICTED_PRICE'] = predicted.cpu().numpy()\nboston_df.head(10)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\n\ntotal_mse = mean_squared_error(boston_df['PRICE'], boston_df['PREDICTED_PRICE'])\nprint(total_mse)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ntr_features, test_features, tr_target, test_target = train_test_split(scaled_features_np, boston_df['PRICE'].values, \n                                                                      test_size=0.3, random_state=2025)\nprint(tr_features.shape, tr_target.shape, test_features.shape, test_target.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tr_features_ts = torch.from_numpy(tr_features)\ntr_targets_ts = torch.from_numpy(tr_target)\n\nw1, w2, bias = gradient_descent(tr_features_ts, tr_targets_ts, iter_epochs=5000, verbose=True)\nprint('##### 최종 w1, w2, bias #######')\nprint(w1.item(), w2.item(), bias.item())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_features_ts = torch.from_numpy(test_features)\ntest_predicted_ts = test_features_ts[:, 0]*w1 + test_features_ts[:, 1]*w2 + bias\n\nboston_test_df = pd.DataFrame({\n    'RM': test_features[:, 0],\n    'LSTAT': test_features[:, 1],\n    'PRICE': test_target,\n    'PREDICTED_PRICE': test_predicted_ts.cpu().numpy()\n})\n\nboston_test_df.head(20)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_total_mse = mean_squared_error(boston_test_df['PRICE'], boston_test_df['PREDICTED_PRICE'])\nprint(test_total_mse)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Stochastic Gradient Descent와 Mini Batch Gradient Descent 구현\n* SGD 는 전체 데이터에서 한건만 임의로 선택하여 Gradient Descent 로 Weight/Bias Update 계산한 뒤 Weight/Bias 적용\n* Mini Batch GD는 전체 데이터에서 Batch 건수만큼 데이터를 선택하여 Gradient Descent로 Weight/Bias Update 계산한 뒤 Weight/Bias 적용","metadata":{}},{"cell_type":"markdown","source":"### SGD 기반으로 Weight/Bias update 값 구하기","metadata":{}},{"cell_type":"code","source":"import torch\n\n# get_update_weights_value() 함수와 거의 유사. \n# 인자로 들어오는 rm_sgd, lstat_sgd, target_sgd은 단 1개의 원소를 가지는 tensor임. \ndef get_update_weights_value_sgd(bias, w1, w2, rm_sgd, lstat_sgd, target_sgd, learning_rate=0.01):\n    # 데이터 건수\n    N = target_sgd.shape[0]\n    # 예측 값. \n    predicted_sgd = w1 * rm_sgd + w2 * lstat_sgd + bias\n    # 실제값과 예측값의 차이\n    diff_sgd = target_sgd - predicted_sgd \n    \n    # weight와 bias를 얼마나 update할 것인지를 계산.  \n    w1_update = -(2/N) * learning_rate * (torch.matmul(rm_sgd, diff_sgd))\n    w2_update = -(2/N) * learning_rate * (torch.matmul(lstat_sgd, diff_sgd))\n    bias_update = -(2/N) * learning_rate * torch.sum(diff_sgd)\n    \n    # weight와 bias가 update되어야 할 값 반환. \n    return bias_update, w1_update, w2_update","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### SGD 수행하기","metadata":{}},{"cell_type":"code","source":"# RM, LSTAT feature tensor와 PRICE target tensor를 입력 받아서 iter_epochs수만큼 반복적으로 Weight와 Bias를 update적용. \ndef st_gradient_descent(features, target, iter_epochs=1000, learning_rate=0.01, verbose=True):\n    # random seed 값 설정. \n    torch.manual_seed(2025)\n    # w1, w2는 1차원 tensor로 변환하되 초기 값은 0으로 설정\n    # bias도 1차원 tensor로 변환하되 초기 값은 1로 설정.\n    w1 = torch.zeros(1, dtype=torch.float32)\n    w2 = torch.zeros(1, dtype=torch.float32)\n    bias = torch.ones(1, dtype=torch.float32)\n    print('최초 w1, w2, bias:', w1.item(), w2.item(), bias.item())\n    \n    # learning_rate와 RM, LSTAT 피처 지정. 호출 시 tensor형태로 RM과 LSTAT으로 된 2차원 feature가 입력됨.\n    rm = features[:, 0]\n    lstat = features[:, 1]\n    \n    # iter_epochs 수만큼 반복하면서 weight와 bias update 수행. \n    for i in range(1, iter_epochs+1):\n        # iteration 시마다 stochastic gradient descent 를 수행할 데이터를 한개만 추출. \n        #추출할 데이터의 인덱스를  로 선택. \n        stochastic_index = torch.randint(0, target.shape[0], size=(1,))\n        rm_sgd = rm[stochastic_index]\n        lstat_sgd = lstat[stochastic_index]\n        target_sgd = target[stochastic_index]\n        # weight/bias update 값 계산. loss 반환 없음. \n        bias_update, w1_update, w2_update = get_update_weights_value_sgd(bias, w1, w2, rm_sgd, lstat_sgd, \n                                                                     target_sgd, learning_rate=0.01)\n        # weight/bias의 update 적용.\n        w1 = w1 - w1_update\n        w2 = w2 - w2_update\n        bias = bias - bias_update\n        if verbose: # 100회 iteration 시마다 출력\n            if i % 100 == 0:\n                print(f'Epoch: {i}/{iter_epochs}')\n                # Loss는 전체 학습 데이터 기반으로 구해야 함. 아래는 전체 학습 feature 기반의 예측 및 loss임.  \n                predicted = w1 * rm + w2*lstat + bias\n                diff = target - predicted\n                loss = torch.mean(diff ** 2)\n                print(f'w1: {w1.item()}, w2: {w2.item()}, bias: {bias.item()}, loss: {loss.item()}')\n        \n    return w1, w2, bias","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\n\n# 학습과 테스트용 feature와 target 분리. \ndef get_scaled_train_test_feature_target_ts(data_df):\n    # RM, LSTAT Feature에 Scaling 적용\n    scaler = MinMaxScaler()\n    scaled_features_np = scaler.fit_transform(data_df[['RM', 'LSTAT']])\n    # 학습 feature, 테스트 feature, 학습 target, test_target으로 분리. \n    tr_features, test_features, tr_target, test_target = train_test_split(scaled_features_np, \n                                                                          data_df['PRICE'].values, \n                                                                          test_size=0.3, random_state=2025)\n    # 학습 feature와 target을 tensor로 변환. \n    tr_ftr_ts = torch.from_numpy(tr_features)\n    tr_tgt_ts = torch.from_numpy(tr_target)\n    test_ftr_ts = torch.from_numpy(test_features)\n    test_tgt_ts = torch.from_numpy(test_target)\n    \n    return tr_ftr_ts, tr_tgt_ts, test_ftr_ts, test_tgt_ts\n\ntr_ftr_ts, tr_tgt_ts, test_ftr_ts, test_tgt_ts = get_scaled_train_test_feature_target_ts(data_df=boston_df)\n\nprint(f\"tr_ftr_ts shape:{tr_ftr_ts.shape} tr_tgt_ts shape:{tr_tgt_ts.shape}\")\nprint(f\"test_ftr_ts shape:{test_ftr_ts.shape} test_tgt_ts shape: {test_tgt_ts.shape}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 학습 feature와 target으로 Stochastic Gradient Descent 수행. \nw1, w2, bias = st_gradient_descent(tr_ftr_ts, tr_tgt_ts, iter_epochs=5000, verbose=True)\nprint('##### 최종 w1, w2, bias #######')\nprint(w1, w2, bias)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 테스트 데이터에서 예측 수행 및 결과를 DataFrame으로 생성. \ntest_predicted_ts = test_ftr_ts[:, 0]*w1 + test_ftr_ts[:, 1]*w2 + bias\n\nboston_test_df = pd.DataFrame({\n    'RM': test_features[:, 0],\n    'LSTAT': test_ftr_ts[:, 1],\n    'PRICE': test_tgt_ts,\n    'PREDICTED_PRICE_SGD': test_predicted_ts.cpu().numpy()\n})\n\nboston_test_df.head(20)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\n\ntest_total_mse = mean_squared_error(boston_test_df['PRICE'], boston_test_df['PREDICTED_PRICE_SGD'])\nprint(test_total_mse)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### iteration시마다 일정한 batch 크기만큼의 데이터를 random하게 가져와서 GD를 수행하는 Mini-Batch GD 수행","metadata":{}},{"cell_type":"code","source":"def get_update_weights_value_batch(bias, w1, w2, rm_batch, lstat_batch, target_batch, learning_rate=0.01):\n    # 데이터 건수\n    N = target_batch.shape[0]\n    # 예측 값. \n    predicted_batch = w1 * rm_batch + w2 * lstat_batch + bias\n    # 실제값과 예측값의 차이\n    diff_batch = target_batch - predicted_batch \n    \n    # weight와 bias를 얼마나 update할 것인지를 계산.  \n    w1_update = -(2/N) * learning_rate * (torch.matmul(rm_batch, diff_batch))\n    w2_update = -(2/N) * learning_rate * (torch.matmul(lstat_batch, diff_batch))\n    bias_update = -(2/N) * learning_rate * torch.sum(diff_batch)\n    \n    # weight와 bias가 update되어야 할 값 반환. \n    return bias_update, w1_update, w2_update","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"batch_indexes = torch.randint(0, 300, size=(30,))\nprint(batch_indexes)\n\ntr_ftr_ts[batch_indexes, 0]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# batch_gradient_descent()는 인자로 batch_size(배치 크기)를 입력 받음. \ndef batch_random_gradient_descent(features, target, iter_epochs=5000, batch_size=30, verbose=True):\n    # random seed 값 설정. \n    torch.manual_seed(2025)\n    # w1, w2는 1차원 tensor로 변환하되 초기 값은 0으로 설정\n    # bias도 1차원 tensor로 변환하되 초기 값은 1로 설정.\n    w1 = torch.zeros(1, dtype=torch.float32)\n    w2 = torch.zeros(1, dtype=torch.float32)\n    bias = torch.ones(1, dtype=torch.float32)\n    print('최초 w1, w2, bias:', w1.item(), w2.item(), bias.item())\n    \n    # learning_rate와 RM, LSTAT 피처 지정. 호출 시 tensor 형태로 RM과 LSTAT으로 된 2차원 feature가 입력됨.\n    learning_rate = 0.01\n    rm = features[:, 0]\n    lstat = features[:, 1]\n    \n    # iter_epochs 수만큼 반복하면서 weight와 bias update 수행. \n    for i in range(1, iter_epochs+1):\n        # batch_size 갯수만큼 데이터를 임의로 선택. \n        batch_indexes = torch.randint(0, target.shape[0], size=(batch_size,))\n        rm_batch = rm[batch_indexes]\n        lstat_batch = lstat[batch_indexes]\n        target_batch = target[batch_indexes]\n        # Batch GD 기반으로 Weight/Bias의 Update를 구함. \n        bias_update, w1_update, w2_update = get_update_weights_value_batch(bias, w1, w2, \n                                                                           rm_batch, lstat_batch, \n                                                                           target_batch, learning_rate)\n        \n        # Batch GD로 구한 weight/bias의 update 적용. \n        w1 = w1 - w1_update\n        w2 = w2 - w2_update\n        bias = bias - bias_update\n        if verbose: # 100회 iteration 시마다 출력\n            if i % 100 == 0:\n                print(f'Epoch: {i}/{iter_epochs}')\n                # Loss는 전체 학습 데이터 기반으로 구해야 함. 아래는 전체 학습 feature 기반의 예측 및 loss임.  \n                predicted = w1 * rm + w2*lstat + bias\n                diff = target - predicted\n                loss = torch.mean(diff ** 2)\n                print(f'w1: {w1.item()}, w2: {w2.item()}, bias: {bias.item()}, loss: {loss.item()}')\n        \n    return w1, w2, bias","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tr_ftr_ts, tr_tgt_ts, test_ftr_ts, test_tgt_ts = get_scaled_train_test_feature_target_ts(data_df=boston_df)\n\n# 학습 feature와 target으로 Stochastic Gradient Descent 수행. \nw1, w2, bias = batch_random_gradient_descent(tr_ftr_ts, tr_tgt_ts, iter_epochs=5000, batch_size=30, verbose=True)\nprint('##### 최종 w1, w2, bias #######')\nprint(w1, w2, bias)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 테스트 데이터에서 예측 수행 및 결과를 DataFrame으로 생성. \ntest_predicted_ts = test_ftr_ts[:, 0]*w1 + test_ftr_ts[:, 1]*w2 + bias\n\nboston_test_df = pd.DataFrame({\n    'RM': test_features[:, 0],\n    'LSTAT': test_ftr_ts[:, 1],\n    'PRICE': test_tgt_ts,\n    'PREDICTED_PRICE_RANDOM_BATCH': test_predicted_ts.cpu().numpy()\n})\n\ntest_total_mse = mean_squared_error(boston_test_df['PRICE'], boston_test_df['PREDICTED_PRICE_RANDOM_BATCH'])\nprint(\"test 데이터 세트의 MSE:\", test_total_mse)\n\nboston_test_df.head(20)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### iteration 시에 순차적으로 일정한 batch 크기만큼의 데이터를 전체 학습데이터에 걸쳐서 가져오는 Mini-Batch GD 수행","metadata":{}},{"cell_type":"code","source":"for batch_step in range(0, 506, 30):\n    print(batch_step)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# batch_gradient_descent()는 인자로 batch_size(배치 크기)를 입력 받음. \ndef batch_gradient_descent(features, target, epochs=300, batch_size=30, verbose=True):\n    # random seed 값 설정. \n    torch.manual_seed(2025)\n    # w1, w2는 1차원 tensor로 변환하되 초기 값은 0으로 설정\n    # bias도 1차원 tensor로 변환하되 초기 값은 1로 설정.\n    w1 = torch.zeros(1, dtype=torch.float32)\n    w2 = torch.zeros(1, dtype=torch.float32)\n    bias = torch.ones(1, dtype=torch.float32)\n    print('최초 w1, w2, bias:', w1.item(), w2.item(), bias.item())\n    \n    # learning_rate와 RM, LSTAT 피처 지정. 호출 시 numpy array형태로 RM과 LSTAT으로 된 2차원 feature가 입력됨.\n    learning_rate = 0.01\n    rm = features[:, 0]\n    lstat = features[:, 1]\n    \n    # iter_epochs 수만큼 반복하면서 weight와 bias update 수행. \n    for i in range(1, epochs+1):\n        # batch_size 만큼 데이터를 가져와서 weight/bias update를 수행하는 로직을 전체 데이터 건수만큼 반복\n        for batch_step in range(0, target.shape[0], batch_size):\n            # batch_size만큼 순차적인 데이터를 가져옴. \n            rm_batch = rm[batch_step:batch_step + batch_size]\n            lstat_batch = lstat[batch_step:batch_step + batch_size]\n            target_batch = target[batch_step:batch_step + batch_size]\n        \n            bias_update, w1_update, w2_update = get_update_weights_value_batch(bias, w1, w2, \n                                                                               rm_batch, lstat_batch, target_batch, \n                                                                               learning_rate)\n            # Batch GD로 구한 weight/bias의 update 적용. \n            w1 = w1 - w1_update\n            w2 = w2 - w2_update\n            bias = bias - bias_update\n        \n        if verbose:\n            print(f'Epoch: {i}/{epochs}')\n            # Loss는 전체 학습 데이터 기반으로 구해야 함. 아래는 전체 학습 feature 기반의 예측 및 loss임.  \n            predicted = w1 * rm + w2*lstat + bias\n            diff = target - predicted\n            loss = torch.mean(diff ** 2)\n            print(f'w1: {w1.item()}, w2: {w2.item()}, bias: {bias.item()}, loss: {loss.item()}')\n        \n    return w1, w2, bias","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tr_ftr_ts, tr_tgt_ts, test_ftr_ts, test_tgt_ts = get_scaled_train_test_feature_target_ts(data_df=boston_df)\n\n# 학습 feature와 target으로 Mini Batch Gradient Descent 수행. \nw1, w2, bias = batch_gradient_descent(tr_ftr_ts, tr_tgt_ts, epochs=300, batch_size=30, verbose=True)\nprint('##### 최종 w1, w2, bias #######')\nprint(w1, w2, bias)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 테스트 데이터에서 예측 수행 및 결과를 DataFrame으로 생성. \ntest_predicted_ts = test_ftr_ts[:, 0]*w1 + test_ftr_ts[:, 1]*w2 + bias\n\nboston_test_df = pd.DataFrame({\n    'RM': test_features[:, 0],\n    'LSTAT': test_ftr_ts[:, 1],\n    'PRICE': test_tgt_ts,\n    'PREDICTED_PRICE_BATCH': test_predicted_ts.cpu().numpy()\n})\n\ntest_total_mse = mean_squared_error(boston_test_df['PRICE'], boston_test_df['PREDICTED_PRICE_BATCH'])\nprint(\"test 데이터 세트의 MSE:\", test_total_mse)\n\nboston_test_df.head(20)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Pytorch를 이용하여 Simple Regression 모델 구축하기\n* Pytorch 모델은 torch.nn.Module 클래스를 상속하여 생성함\n* nn.Parameter()는 학습 파라미터(Learnable Parameter) tensor를 생성\n* Pytorch의 train 로직은 model의 출력값(feed forward)을 오차 역전파(Backpropagation)로 weight 값 update 수행\n* 손실 함수는 nn.MSELoss()로, Optimizer는 Adam 생성하고 모델 학습 수행.","metadata":{}},{"cell_type":"code","source":"list(simple_model_01.parameters())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nclass SimpleRegression_01(nn.Module):\n    def __init__(self):\n        super().__init__()\n        #w\n        self.weights = nn.Parameter(data=torch.zeros(size=(2, ), dtype=torch.float32),\n                                   requires_grad=True)\n        self.bias = nn.Parameter(data=torch.ones(size=(1,), dtype=torch.float32))\n        \n    def forward(self, x):\n        return torch.matmul(self.weights, x.t()) + self.bias # w1*x1 + w2*x2 + b\n\nsimple_model_01 = SimpleRegression_01()\n# simple_model의 학습 파라미터 출력(Learnable Parameter)\nprint(list(simple_model_01.parameters()))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_scaled_train_test_feature_target_ts_01(data_df):\n    # RM, LSTAT Feature에 Scaling 적용\n    scaler = MinMaxScaler()\n    scaled_features_np = scaler.fit_transform(data_df[['RM', 'LSTAT']])\n    # 학습 feature, 테스트 feature, 학습 target, test_target으로 분리. \n    tr_features, test_features, tr_target, test_target = train_test_split(scaled_features_np, \n                                                                          data_df['PRICE'].values, \n                                                                          test_size=0.3, random_state=2025)\n    # 학습 feature와 target을 tensor로 변환. dtype=torch.float32로 수정\n    tr_ftr_ts = torch.tensor(tr_features, dtype=torch.float32)\n    tr_tgt_ts = torch.tensor(tr_target, dtype=torch.float32)\n    test_ftr_ts = torch.tensor(test_features, dtype=torch.float32)\n    test_tgt_ts = torch.tensor(test_target, dtype=torch.float32)\n    \n    return tr_ftr_ts, tr_tgt_ts, test_ftr_ts, test_tgt_ts","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tr_ftr_ts, tr_tgt_ts, test_ftr_ts, test_tgt_ts = get_scaled_train_test_feature_target_ts_01(data_df=boston_df)\nprint(tr_ftr_ts.dtype, tr_tgt_ts.dtype, test_ftr_ts.dtype, test_tgt_ts.dtype)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# MSELoss 생성. \nloss_fn = nn.MSELoss(reduction='mean')\n# optimizer는 Adam으로, 생성 시 인자로 model의 모든 parameter 값과 learning rate가 필요. \noptimizer = torch.optim.Adam(simple_model_01.parameters(), lr=0.01)\n\n# train loop 수행. \ndef train_loop(model, tr_ftr_ts, tr_tgt_ts, loss_fn, optimizer, epochs=300, batch_size=30, verbose=True):\n    #model.train()\n    for i in range(1, epochs+1):\n    # batch_size 만큼 데이터를 가져와서 weight/bias update를 수행하는 로직을 전체 데이터 건수만큼 반복\n        for batch_step in range(0, tr_tgt_ts.shape[0], batch_size):\n            # batch_size만큼 순차적인 데이터를 가져옴.\n            ftr_batch = tr_ftr_ts[batch_step:batch_step + batch_size]\n            target_batch = tr_tgt_ts[batch_step:batch_step + batch_size]\n            \n            # forward pass\n            output = model(ftr_batch).squeeze(-1)\n            \n            # mse loss 계산\n            loss = loss_fn(output, target_batch)\n\n            # backward pass\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            if verbose:\n                if batch_step == 330:\n                    print(f'Epoch: {i}/{epochs}, batch step:{batch_step}, loss: {loss.item()}')\n\n    return model","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"simple_model_01 = SimpleRegression_01()\nloss_fn = nn.MSELoss(reduction='mean')\noptimizer = torch.optim.Adam(simple_model_01.parameters(), lr=0.01)\n\ntrained_model = train_loop(simple_model_01, tr_ftr_ts, tr_tgt_ts, loss_fn, optimizer, \n                         epochs=300, batch_size=30, verbose=True) ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\n\ntest_predicted_ts = trained_model(test_ftr_ts)\nprint(test_predicted_ts.requires_grad, test_predicted_ts.shape)\n\nboston_test_df = pd.DataFrame({\n    'RM': test_features[:, 0],\n    'LSTAT': test_ftr_ts[:, 1],\n    'PRICE': test_tgt_ts,\n    'PREDICTED': test_predicted_ts.squeeze(-1).detach().numpy()\n})\n\ntest_total_mse = mean_squared_error(boston_test_df['PRICE'], boston_test_df['PREDICTED'])\nprint(\"test 데이터 세트의 MSE:\", test_total_mse)\n\nboston_test_df.head(20)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nclass SimpleRegression_02(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = nn.Linear(in_features=2, out_features=1, bias=True)\n\n    def forward(self, x):\n        return self.linear(x)\n\nsimple_model_02 = SimpleRegression_02()\n# simple_model의 학습 파라미터 출력(Learnable Parameter)\nprint(list(simple_model_02.parameters()))\nprint(simple_model_02)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"loss_fn = nn.MSELoss(reduction='mean')\noptimizer = torch.optim.Adam(simple_model_02.parameters(), lr=0.01)\ntr_ftr_ts, tr_tgt_ts, test_ftr_ts, test_tgt_ts = get_scaled_train_test_feature_target_ts_01(data_df=boston_df)\n\ntrained_model = train_loop(simple_model_02, tr_ftr_ts, tr_tgt_ts, loss_fn, optimizer, \n                         epochs=300, batch_size=30, verbose=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\n\ntest_predicted_ts = trained_model(test_ftr_ts)\nprint(test_predicted_ts.requires_grad, test_predicted_ts.shape)\n\nboston_test_df = pd.DataFrame({\n    'RM': test_features[:, 0],\n    'LSTAT': test_ftr_ts[:, 1],\n    'PRICE': test_tgt_ts,\n    'PREDICTED': test_predicted_ts.squeeze(-1).detach().numpy()\n})\n\ntest_total_mse = mean_squared_error(boston_test_df['PRICE'], boston_test_df['PREDICTED'])\nprint(\"test 데이터 세트의 MSE:\", test_total_mse)\n\nboston_test_df.head(20)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 활성화 함수(Activation Function)\n* function 기반으로 또는 Layer 기반으로 적용 가능\n* relu()/ReLU()는 inplace=False(default)시 원본 입력 tensor를 복제하여 relu 적용. inplace=True시 원본 입력 tensor에 바로 relu를 적용하므로 원본 입력 tensor가 변환됨(메모리가 절감됨)\n* softmax()/Softmax()의 경우는 dim=-1 을 보통 적용함","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\ninput_tensor = torch.tensor([0, 9.0, 10.0, -9.0, -10.0], dtype=torch.float32)\n\n# sigmoid 적용\nsigmoid_output = F.sigmoid(input_tensor)\n# sigmoid_output = torch.sigmoid(input_tensor)\n\nprint('sigmoid_output:', sigmoid_output)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Sigmoid Layer 적용\nsigmoid_layer = nn.Sigmoid()\nsigmoid_output = sigmoid_layer(input_tensor)\nprint('sigmoid_output_02:', sigmoid_output)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# relu() 함수 적용\ninput_tensor = torch.tensor([0, 9.0, 10.0, -9.0, -10.0], dtype=torch.float32)\n# relu function 적용\n# inplace=False시 입력 원본 tensor는 그대로 유지한 채 입력 tensor를 복사하여 relu함수 적용하여 반환\n# inplace=True시 입력 원본 tensor에 바로 relu함수 적용하여 반환. 입력 tensor가 바로 relu 변환됨. 메모리 절감 \nrelu_output = F.relu(input_tensor, inplace=False) # inplace=True\nprint('relu_output:', relu_output)\n# inplace=False시 input_tensor 값은 변환 없음. inplace=True시 input_tensor 값이 변화됨. \nprint('input_tensor:', input_tensor)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ReLU Layer 적용\ninput_tensor = torch.tensor([0, 9.0, 10.0, -9.0, -10.0], dtype=torch.float32)\nrelu_layer = nn.ReLU() #inplace=True\nrelu_output = relu_layer(input_tensor)\nprint('relu_output:', relu_output)\nprint('input_tensor:', input_tensor)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# softmax 함수 적용\ninput_tensor = torch.tensor([[1.0, -1.0, 0.5], \n                             [0.5, 0.5, 1.5]], dtype=torch.float32)\nprint('input_tensor shape:', input_tensor.shape)\nsoftmax_output = F.softmax(input_tensor, dim=-1)\n#softmax_output = torch.softmax(input_tensor, dim=-1)\nprint('softmax_output:', softmax_output)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Custom Model에서 Activation Function 사용","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n#from torch import nn\n\n# Custom Model 생성. \nclass LinearModel(nn.Module):\n    def __init__(self):\n        # 반드시 super()를 호출. \n        super().__init__()\n        #Linear Layer와 ReLU Layer 생성. \n        self.linear_01 = nn.Linear(in_features=10, out_features=5)\n        self.relu_01 = nn.ReLU()\n        self.linear_02 = nn.Linear(in_features=5, out_features=3)\n        \n    # 순방향 전파(Pass Forward) 기술.\n    def forward(self, x):\n        x = self.linear_01(x)\n        x = self.relu_01(x)\n        output = self.linear_02(x)\n        return output","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#임의의 입력 tensor 생성. \ninput_tensor = torch.randn(size=(4, 10))\nprint(input_tensor)\n\nlinear_model = LinearModel()\n\n# LinearModel 객체는 Callable Object이므로 LinearModel 객체에 함수 호출과 유사한 형태로 입력 인자 전달하여 forward()메소드 호출. \noutput_tensor = linear_model(input_tensor)\nprint(output_tensor)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"softmax_output = F.softmax(output_tensor, dim=-1)\n#softmax_output = torch.softmax(input_tensor, dim=-1)\nprint('softmax_output:', softmax_output)\nprint('predicted class:', softmax_output.argmax(dim=-1))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}