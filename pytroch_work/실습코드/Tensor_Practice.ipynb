{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Pytorch Tensor 생성, shape, 차원, 타입\n* torch Tensor는 scalar값, python list, numpy array, 기존 tensor등을 기반으로 생성할 수 있음.\n* tensor의 형태(shape)는 shape 속성 또는 size() 메소드로 확인 ","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport torch\n\nlist_01 = [1, 2, 3]\nts_01 = torch.tensor(list_01)\nts_02 = torch.tensor([[1, 2, 3],\n                      [2, 3, 4]\n                     ])\nts_03 = torch.tensor([\n                      [[1, 2, 3],\n                       [2, 3, 4]],\n                      [[3, 4, 5],\n                       [4, 5, 6]]\n                     ]) \nprint('ts_01:', ts_01.shape, 'ts_02 shape:', ts_02.shape, 'ts_03 shape:', ts_03.shape) ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"np.array([1, 2, 3]).shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Tensor의 shape/size 및 차원\n* pytorch tensor는 shape 속성 및 size() 메소드를 통해 torch.Size 오브젝트를 반환하여 tensor의 형태(shape)를 제공\n* shape 속성과 size() 메소드는 거의 동일.\n* 순수하게 차원 정보만 알고 싶을 때는 ndim 속성을 이용","metadata":{}},{"cell_type":"code","source":"print(ts_02.shape, ts_02.size())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(ts_02.shape[0], ts_02.shape[1], ts_02.size(0), ts_02.size(1), ts_02.size()[0]) ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ts_02.ndim","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### tensor 데이터 타입\n* tensor내의 값은 동일한 데이터 타입을 가짐. dtype 속성으로 확인\n* 데이터 타입은 생성시에 지정하거나, type() 또는 to() 메소드를 이용하여 변환. ","metadata":{}},{"cell_type":"code","source":"ts_01 = torch.tensor([1.0, 2, 3])\nprint(ts_01.dtype)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ts_01 = torch.tensor([1, 2, 3], dtype=torch.float32)\nprint(ts_01.dtype)  # torch.float32","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ts_01_1 = ts_01.int() #int32로 변환\nprint(ts_01_1.dtype)\n\nts_01_2 = ts_01.float() #float32 변환\nprint(ts_01_2.dtype)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ts_01_1 = ts_01.type(torch.int64)\nprint(ts_01_1.dtype)\n\nts_01_2 = ts_01.type(torch.int8) # float32/float64\nprint(ts_01_2.dtype)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ts_01_1 = ts_01.to(torch.int64)\nprint(ts_01_1.dtype)\n\nts_01_2 = ts_01.to(torch.int8) # float32/float64\nprint(ts_01_2.dtype)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### numpy array와 pytorch tensor간 상호 변환","metadata":{}},{"cell_type":"code","source":"# numpy array를 tensor로 변환. \narr_01 = np.array([1, 2])\nts_01 = torch.tensor(arr_01)\nts_02 = torch.from_numpy(arr_01)\nprint(type(arr_01), ts_01, ts_02)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# tensor를 array 또는 list로 변환. \narr_01_1 = ts_01.numpy()\nlist_01 = ts_01.tolist()\nprint(arr_01_1, type(arr_01_1), list_01, type(list_01))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport torch\n\nts_01 = torch.tensor([1, 2])\nts_01_1 = ts_01.to('cuda')\n# 아래는 오류를 발생. device가 cuda에 있는 tensor의 경우 numpy() 메소드를 바로 호출 할 수 없음.\narr_01_1 = ts_01_1.numpy() #  ts_01_1.cpu().numpy()를 호출해야 함. ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# tensor.from_numpy(array) 수행 시 생성된 tensor는 입력된 array와 메모리를 공유\n# 입력 array가 변경될 경우 tensor도 같이 변경됨.\narr_01 = np.array([1, 2])\nts_01 = torch.from_numpy(arr_01)\nprint('arr_01:', arr_01, 'ts_01:', ts_01)\n\narr_01[0] = 0\nprint('arr_01:', arr_01, 'ts_01:', ts_01)\n\n# clone() 을 사용하여 tensor를 복제\nts_02 = ts_01.clone()\narr_01[0] = 100\nprint('arr_01:', arr_01, 'ts_01:', ts_01, 'ts_02:', ts_02)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### tensor를 편리하게 생성하기 - arange, zeros, ones","metadata":{}},{"cell_type":"code","source":"seq_ts = torch.arange(10)\nprint(seq_ts)\nprint(seq_ts.dtype, seq_ts.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"torch.zeros(size=(3, 2), dtype=torch.int32).dtype","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"zero_ts = torch.zeros(size=(3, 2), dtype=torch.int32)  #문자열 'int32'는 안됨. \nprint(zero_ts)\nprint(zero_ts.dtype, zero_ts.shape)\n\none_ts = torch.ones(3, 2, dtype=torch.int16) # tuple을 사용하지 않아도 됨\nprint(one_ts)\nprint(one_ts.dtype, one_ts.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### random 값 생성하기\n* rand()은 기본으로 0 ~ 1(1은 제외)사이의 균일 분포(uniform distribution) random 값을 생성.\n* randint()는 random 정수값을 생성\n* randn()은 정규 분포(Normal distribution) random 값을 생성. ","metadata":{}},{"cell_type":"code","source":"torch.manual_seed(2025)  # random 수행 시 마다 동일 값 생성을 위한 seed값 할당. \n\nts_01 = torch.rand(size=(3, 4)) # 0 ~ 1 사이의 float32 random값. 1제외\nprint('ts_01:\\n', ts_01, ts_01.dtype)\nprint(ts_01.min(), ts_01.max())\n\nts_01 = torch.randint(low=0, high=100, size=(3, 4)) # 0 ~ 99까지의 random value\nprint('ts_01:\\n', ts_01, ts_01.dtype)\nprint(ts_01.min(), ts_01.max())\n\nts_01 = torch.randn(size=(3, 4)) # 평균이 0 이고, 분산이 1인 정규 분포 random value. \nprint('ts_01:\\n', ts_01, ts_01.dtype)\nprint(ts_01.min(), ts_01.max(), ts_01.mean(), ts_01.var())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### tensor의 형태(shape)를 변경하는 reshape()와 view()\n* reshape()와 view() 모두 tensor의 형태를 변환하나 view()는 contiguous memory 구조에서만 동작됨.\n* pytorch는 reshape()보다는 view() 호출을 권장. (강제로) tensor에서 contiguous memory 구조를 유지하기를 권장함.","metadata":{}},{"cell_type":"code","source":"import torch \n\nts_01 = torch.arange(10)\nprint('ts_01:\\n', ts_01)\n\nts_02 = ts_01.reshape((2, 5))\nprint('ts_02:\\n',ts_02)\n\nts_03 = ts_01.reshape(5, 2)\nprint('ts_03:\\n',ts_03)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"### 아래는 오류 발생 \nts_01.reshape(3, 4)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ts_02 = ts_01.view((2, 5))\nprint('ts_02:\\n', ts_02, ts_02.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"torch.manual_seed(2025) \n\nts_01 = torch.rand(size=(16, 3, 32, 32))\nts_02 = ts_01.view(16, -1)\nprint(ts_02.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#기본적으로 tensor는 contiguous memory 구조로 생성됨. \nts_01 = torch.arange(10)\nts_02 = ts_01.view(2, -1)\nts_03 = ts_01.reshape(2, -1)\n\nprint(ts_01.is_contiguous(), ts_02.is_contiguous(), ts_03.is_contiguous())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### tensor의 차원 위치를 변경하여 재배열하는 permute(), t(), transpose()\n* permute()는 차원의 위치를 자유롭게 변경할 수 있음.\n* t()는 2차원 tensor의 row와 col을 변경.\n* transpose()는 2개 차원의 위치만 변경할 수 있음.\n* permute(), t(), transpose() 수행 시 contiguous memory가 구조가 깨지므로 view() 적용 시 유의 필요","metadata":{}},{"cell_type":"code","source":"torch.manual_seed(2025)\n\nts_01 = torch.rand(size=(64, 64, 3))\nprint(ts_01)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"torch.manual_seed(2025)\n\nts_01 = torch.rand(size=(64, 64, 3))\nprint(ts_01.shape)\n\nts_02 = ts_01.permute(dims=(2, 0, 1)) # torch.permute(ts_01, dims=(2, 0, 1))\nprint(ts_02.shape)\n\nts_03 = ts_02.permute(dims=(1, 2, 0)) # torch.permute(ts_02, dims=(1, 2, 0))\nprint(ts_03.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ts_01 = torch.rand(size=(16, 32, 32, 3))\nts_02 = ts_01.permute(dims=(0, 3, 1, 2))\nprint(ts_02.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ts_01 = torch.rand(size=(3, 64, 64))\nts_02 = torch.permute(ts_01, dims=(1, 2, 0)) #permute로 차원 이동 시 연속(contiguous) 메모리 구조가 깨질 수 있음.\nprint('ts_02 shape:', ts_02.shape)\nprint('is ts_02 contiguous? ', ts_02.is_contiguous())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ts_02_1 = ts_02.view(64, -1)# permute로 차원 이동 시 연속(contiguous) 메모리 구조가 깨질 수 있음. 이 경우 view를 사용하면 안됨.","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ts_01 = torch.rand(size=(3, 64, 64))\nts_02 = torch.permute(ts_01, dims=(1, 2, 0))\n\nts_02_1 = ts_02.reshape(64, -1) # reshape는 연속 메모리 구조와 관계없이 변환\nprint('ts_02_1 shape:', ts_02_1.shape)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ts_01 = torch.rand(size=(3, 64, 64))\nts_02 = torch.permute(ts_01, dims=(1, 2, 0))\nprint('ts_02 shape:', ts_02.shape)\n\nts_02_1 = ts_02.contiguous().view(64, -1) # contiguous()로 연속 메모리 구조 만든 후 view() 적용.","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ts_01 = torch.rand(size=(3, 4))\nts_02 = ts_01.t()\nprint('ts_02 shape:', ts_02.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ts_01 = torch.rand(size=(64, 3, 128, 248))\nts_02 = ts_01.transpose(2, 3)\nprint('ts_02 shape:', ts_02.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### tensor에 sum, max, min, mean등의 aggregation 적용\n* sum, max, min, mean등의 aggregation 메소드는 dim 인자를 가짐. dim이 None일 경우 전체 원소들에 aggregation 적용.\n* dim인자는 해당 차원(axis 축)을 따라서(방향성으로) aggregation을 수행.\n* 단일 차원의 dim인자를 적용하여 aggregation을 수행 할 경우 반환되는 tensor의 차원수는 원본 tensor의 차원 수 - 1임.\n* 여러 차원의 dim인자를 적용하여 aggregation을 수행 할 경우 반환되는 tensor의 차원수는 원본 tensor의 차원 수 - 해당 dim 차원수임. ","metadata":{}},{"cell_type":"code","source":"ts_01 = torch.arange(10).view(2, 5)\nts_01.max()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch \n\nts_01 = torch.arange(10).view(2, 5)\nprint(ts_01)\n\nprint('total sum:', ts_01.sum())\nprint('sum along dim=0:', ts_01.sum(dim=0))\nprint('sum along dim=1:', ts_01.sum(dim=1))\n\nprint('max overall:', ts_01.max())\nprint('max along dim=0:', ts_01.max(dim=0))# max값과 max가 위치한 index값을 함께 반환. \nprint('max along dim=1:', ts_01.max(dim=1))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(ts_01)\nprint('overall mean:', ts_01.mean(dtype=torch.float64))\nprint('mean along dim=0:', ts_01.mean(dim=0, dtype=torch.float64))\nprint('mean along dim=1:', ts_01.mean(dim=1, dtype=torch.float64))\n\nprint('min overall:', ts_01.min())\nprint('min along dim=0:', ts_01.min(dim=0))# min값과 min이 위치한 index값을 함께 반환. \nprint('min along dim=1:', ts_01.min(dim=1))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"torch.arange(24).reshape(2, 3, 4)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ts_02 = torch.arange(24).reshape(2, 3, 4)\nprint(ts_02)\n\nprint('total sum:', ts_02.sum())\nprint('sum along dim=0:\\n', ts_02.sum(dim=0))\nprint('sum along dim=1:\\n', ts_02.sum(dim=1))\nprint('sum along dim=2:\\n', ts_02.sum(dim=2))\nprint('sum along dim=-1:\\n', ts_02.sum(dim=-1))\n\nprint('max overall:', ts_02.max())\nprint('max along dim=0:', ts_02.max(dim=0))# max값과 max가 위치한 index값을 함께 반환. \nprint('max along dim=1:', ts_02.max(dim=1))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(ts_02)\nprint('sum along dim=(1, 2):\\n', ts_02.sum(dim=(1, 2)))\nprint('sum along dim=(2, 1):\\n', ts_02.sum(dim=(2, 1)))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(ts_02)\nprint('sum along dim=(0, 1):\\n', ts_02.sum(dim=(0, 1)))\nprint('sum along dim=(1, 0):\\n', ts_02.sum(dim=(1, 0)))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(ts_02)\nprint('sum along dim=(-1):\\n', ts_02.sum(dim=(-1)))\nprint('sum along dim=(-2, -1):\\n', ts_02.sum(dim=(-2, -1)))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### argmax() 수행\n* max()는 tensor내의 가장 큰 값을 반환하지만, argmax()는 가장 큰값을 가지는 index를 반환. ","metadata":{}},{"cell_type":"code","source":"torch.manual_seed(2025)  # random 수행 시 마다 동일 값 생성을 위한 seed값 할당. \n\nts_01 = torch.rand(size=(10,))\nprint(ts_01)\nprint(ts_01.max(), ts_01.argmax())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"torch.manual_seed(2025)\n\nts_01 = torch.rand(size=(4, 5))\nprint(ts_01)\nprint(ts_01.argmax(dim=-1))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# dim값이 입력된 max()는 max값과 max위치의 index를 반환\nprint(ts_01.max(dim=-1))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"_, val_index = ts_01.max(dim=-1)\nprint( val_index)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(ts_01.argmax(dim=0))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### squeeze()와 unsqueeze()\n* squeeze()는 1로 되어 있는 차원값을 없앤 tensor반환. unsqueeze는 기존 차원을 1차원 증가시킴.\n* squeeze(dim=0)과 같이 해당 dim을 지정하면 해당 차원값만(1로 되어 있어야 함) 삭제하여 재 구성한 tensor 반환\n* squeeze(dim=None)은 1로 되어 있는 차원값을 모두 찾아서 없앤 tensor 반환.\n* unsqueeze()는 dim=0과 같이 반드시 dim인자를 입력해 줘야 함(squeeze()도 가급적이면 반드시 dim인자 입력)","metadata":{}},{"cell_type":"code","source":"torch.manual_seed(2025)\n\nts_01 = torch.rand(size=(1, 4, 5))\nprint(ts_01)\n\nts_01_1 = ts_01.squeeze()\nprint('ts_01 shape:', ts_01.shape, 'ts_01 squeezed shape:', ts_01_1.shape)\nprint(ts_01_1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# squeeze(dim=0)과 같이 dim 인자 적용\nts_01 = torch.rand(size=(1, 4, 1))\nts_01_1 = ts_01.squeeze(dim=0)\nts_01_2 = ts_01.squeeze(dim=-1)\nts_01_3 = ts_01.squeeze(dim=(0, -1))\n\nprint(ts_01.shape, ts_01_1.shape, ts_01_2.shape, ts_01_3.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 보통은 batch가 포함된 4차원 이미지 tensor에서 batch가 1일 경우 3차원 단일 이미지 tensor로 변환하는데 주로 사용 \nts_01 = torch.rand(size=(1, 3, 64, 64))\nts_01_1 = ts_01.squeeze(dim=0)\nprint('ts_01 shape:', ts_01.shape, 'ts_01 squeezed shape:', ts_01_1.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"torch.manual_seed(2025)\n\nts_01 = torch.rand(size=(4, 5))\nprint(ts_01)\n\nts_01_1 = ts_01.unsqueeze(dim=0) # dim 인자가 반드시 사용되어야 함. \nprint('ts_01 shape:', ts_01.shape, 'ts_01 unsqueezed shape:', ts_01_1.shape)\nprint(ts_01_1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 보통은 3차원 단일 이미지 tensor를 batch 포함 4차원 이미지 tensor로 변환 시 자주 사용\nts_01 = torch.rand(size=(3, 64, 64))\nts_01_1 = ts_01.unsqueeze(dim=0)\nprint('ts_01 shape:', ts_01.shape, 'ts_01 unsqueezed shape:', ts_01_1.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"torch.tensor([1])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### item() \n* tensor 자체가 아니라 tensor값만 반환하는 데 사용. tensor가 값을 1개만 가지고 있을 경우 이 값을 python scalar 값으로 반환\n* 1차원 tensor에 값이 하나만 있거나 tensor가 scalar type일 경우만 item() 가능.","metadata":{}},{"cell_type":"code","source":"ts_01 = torch.tensor([1])\nprint(ts_01.dtype, ts_01.shape, ts_01.ndim)\nprint('ts_01 item():', ts_01.item())\n\nts_02 = torch.tensor(1)\nprint(ts_02.dtype, ts_02.shape, ts_02.ndim)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 아래는 1차원 이지만 값이 2개 이상이므로 item() 호출 시 오류 발생. \nts_01 = torch.tensor([1, 2])\nprint(ts_01.dtype, ts_01.shape, ts_01.ndim)\nprint('ts_01 item():', ts_01.item())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### indexing\n* tensor의 indexing은 numpy array indexing과 매우 유사\n* 단일 지정(integer) indexing, 슬라이싱(:) indexing, Fancy(List) indexing, Boolean indexing 외에도 다양한 indexing을 제공\n* 단일 지정 indexing을 수행 할 경우 원본 tensor를 한 차원 줄인 tensor반환(numpy array도 마찬가지)\n* numpy와 다르게 pytorch boolean indexing을 1차원 tensor를 반환(numpy array는 원본 차원 유지)\n","metadata":{}},{"cell_type":"code","source":"ts_01 = torch.arange(0, 10).view(2, 5)\nprint(ts_01)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 지정 인덱싱 적용\nprint('ts_01[0, 0]:', ts_01[0, 0], 'ts_01[0, 1]:', ts_01[0, 1])\nprint('ts_01[1, 0]:', ts_01[1, 0], 'ts_01[1, 2]:', ts_01[1, 2])\nprint(ts_01[0, 0].shape, ts_01[0, 0].ndim, ts_01[0, :].shape, ts_01[0, :].ndim)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(ts_01)\nprint('ts_01[0, :]은', ts_01[0, :], 'ts_01[:, 0]은', ts_01[:, 0])\nprint('ts_01[0, 0:3]은', ts_01[0, 0:3], 'ts_01[1, 1:4]은', ts_01[1, 1:4])\nprint('ts_01[:, :]\\n', ts_01[:, :])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"torch.manual_seed(2025)\n\nrandom_indexes = torch.randint(0, 5, size=(4,))\nprint('random_indexes:', random_indexes)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"torch.manual_seed(2025)\n\nrandom_indexes = torch.randint(0, 5, size=(4,))\nprint('random_indexes:', random_indexes)\n\nts_01 = torch.rand(size=(10, 5))\nprint('ts_01:\\n', ts_01)\n\nts_01_1 = ts_01[random_indexes] # fancy indexing 적용\nprint('ts_01_1:\\n', ts_01_1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ts_01 = torch.arange(0, 10).view(2, 5)\nprint(ts_01)\n\nmask = ts_01 > 4\nprint(mask)\nprint(ts_01[mask]) # pytorch tensor의 boolean indexing은 1차원 tensor를 반환.","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# where는 원본 배열 차원을 보존\ntorch.where(ts_01 > 4, input=ts_01, other=torch.tensor(999))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### 내적과 행렬곱 연산 - dot()와 matmul()\n* torch의 dot() 연산은 1차원 tensor만 가능(vector dot product)\n* torch의 matmul() 연산은 1차원-2차원 tensor간, 2차원 tensor간(matrix) 행렬곱 연산 수행\n* matmul()의 경우 3차원 이상의 tensor가 입력될 경우 맨 뒤의 2개 차원을 행렬로, 앞의 차원들은 batch로 간주하고 행렬곱 연산 수행\n![행렬곱](https://github.com/chulminkw/CNN_PG_Torch/blob/main/image/matmul.png?raw=true)","metadata":{}},{"cell_type":"code","source":"import torch \n\nts_01 = torch.arange(1, 4)\nts_02 = torch.arange(4, 7)\nprint('ts_01:', ts_01, 'ts_02:', ts_02)\n\nts_03 = torch.dot(ts_01, ts_02) # dot()연산은 1차원끼리만 가능. \nprint('ts_03:', ts_03)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ts_01 = torch.arange(1, 7).view(2, 3)\nts_02 = torch.arange(7, 13).view(3, 2)\nprint('ts_01:\\n', ts_01, '\\n', 'ts_02:\\n', ts_02)\n\nts_03 = torch.matmul(ts_01, ts_02) # 2차원 행렬 곱 연산 수행\nprint('ts_03:\\n', ts_03)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 3차원 이상의 행렬 곱. batch size는 서로 동일해야 함. 단 둘중 한개가 batch size 1인 경우는 상관 없음. \nts_01 = torch.arange(0, 24).view(2, 3, 4) # batch size 2를 가지는 3x4 행렬\nts_02 = torch.arange(0, 40).view(2, 4, 5)  # batch size 2을 가지는 4x5 행렬\nprint('ts_01:\\n', ts_01, '\\n', 'ts_02:\\n', ts_02)\n\nts_03 = torch.matmul(ts_01, ts_02)  # 맨 앞차원은 배치로 간주하고 뒤 차원 2개를 행렬 곱 연산 수행. \nprint('ts_03:\\n', ts_03)\nprint(ts_03.shape) # 출력 shape는 (2, 3, 5)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"torch.matmul(\ntorch.tensor(\n[[ 0,  1,  2,  3],\n[ 4,  5,  6,  7],\n[ 8,  9, 10, 11]]), \ntorch.tensor(\n[[ 0,  1,  2,  3,  4],\n[ 5,  6,  7,  8,  9],\n[10, 11, 12, 13, 14],\n[15, 16, 17, 18, 19]]\n))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# batch size는 서로 동일해야 함. 단 둘중 한개가 batch size 1인 경우는 상관 없음.\nts_01 = torch.arange(0, 24).view(2, 3, 4) # batch size 2를 가지는 3x4 행렬\nts_02 = torch.arange(0, 60).view(3, 4, 5)  # batch size=3을 가지는 4x5 행렬\nprint('ts_01:\\n', ts_01, '\\n', 'ts_02:\\n', ts_02)\n\nts_03 = torch.matmul(ts_01, ts_02)  # 맨 앞차원은 배치로 간주하고 뒤 차원 2개를 행렬 곱 연산 수행. \nprint(ts_03)\nprint(ts_03.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}